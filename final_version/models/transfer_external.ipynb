{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Change working directory to the parent directory\n",
    "os.chdir(\"/Users/megan/Thesis\")\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv('data/data_with_keywords.csv')\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "\n",
    "# Load the keywords CSV file into a DataFrame\n",
    "keywords_df = pd.read_csv('data/external/keywords_trends.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the data\n",
    "keywords_df.head()\n",
    "\n",
    "keywords_list = pd.read_csv('data/external/keywords.csv')['Keyword'].tolist()\n",
    "\n",
    "def get_trends_file(keyword, trends_folder='data/external/google_trends_v2'):\n",
    "    \"\"\"\n",
    "    keyword: A single string (e.g. \"Bag\" or \"Bag Beige Solid\")\n",
    "    Returns the path if the file exists, else prints a warning and returns None.\n",
    "    \"\"\"\n",
    "    file_name = f\"{keyword.replace(' ', '_')}_trend_data.csv\"\n",
    "    file_path = os.path.join(trends_folder, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        return file_path\n",
    "    else:\n",
    "        print(f\"Warning: Trend file for {keyword} not found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_and_process_trends(trend_file, product_data):\n",
    "    if 'date' in product_data.columns:\n",
    "        product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "        product_data.set_index('date', inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Load trends\n",
    "    trend_data = pd.read_csv(trend_file)\n",
    "    trend_data.drop(columns=['isPartial'], inplace=True)\n",
    "    trend_data['date'] = pd.to_datetime(trend_data['date'], errors='coerce')\n",
    "    trend_data['date'] = trend_data['date'] + pd.Timedelta(weeks=36)\n",
    "    trend_data.set_index('date', inplace=True)\n",
    "\n",
    "    trend_data = trend_data.reindex(product_data.index, method='ffill')\n",
    "    return trend_data\n",
    "\n",
    "\n",
    "def add_trends_to_product_data(product_data, trend_data, trend_column_name):\n",
    "    \"\"\"\n",
    "    Adds the trend data to the product_data based on the trend column name.\n",
    "\n",
    "    Parameters:\n",
    "    product_data (pd.DataFrame): The product data.\n",
    "    trend_data (pd.DataFrame): The trend data containing the trend values.\n",
    "    trend_column_name (str): The name of the column to store the trend data in product_data.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The updated product data with the trend column added.\n",
    "    \"\"\"\n",
    "    # Add the trend data to the product_data DataFrame\n",
    "    product_data[trend_column_name] = trend_data[trend_column_name]  # Add dynamic trend column\n",
    "\n",
    "    return product_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, input_window=14, output_window=14, target_col_name='transaction_count'):\n",
    "        \"\"\"\n",
    "        df               : A pandas DataFrame that includes both features and the target column.\n",
    "        input_window     : Number of time steps used as input (lookback).\n",
    "        output_window    : Number of time steps to forecast (e.g. 14 for 14-day).\n",
    "        target_col_name  : Name of the target column in df.\n",
    "        \"\"\"\n",
    "        # Separate the features vs. target\n",
    "        self.X_data = df.values  # all columns except target\n",
    "        self.y_data = df[target_col_name].values               \n",
    "\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "\n",
    "    def __len__(self):\n",
    "        # The maximum valid index is total_rows - (input_window + output_window)\n",
    "        return len(self.X_data) - (self.input_window + self.output_window - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Slice out the input window\n",
    "        x_start = idx\n",
    "        x_end   = idx + self.input_window\n",
    "        X = self.X_data[x_start : x_end]   # shape => (input_window, num_features)\n",
    "\n",
    "        # 2) Slice out the next 'output_window' points of the target\n",
    "        y_start = x_end\n",
    "        y_end   = x_end + self.output_window\n",
    "        Y = self.y_data[y_start : y_end]   # shape => (output_window,)\n",
    "\n",
    "        # Convert to float32 for PyTorch\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.float32)\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                              stride=1, padding=padding, dilation=dilation)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, channels, seq_len]\n",
    "        \"\"\"\n",
    "        out = self.conv(x)\n",
    "        # Remove extra time-steps from padding to maintain causality\n",
    "        out = out[:, :, :-self.conv.padding[0]]  # remove the last \"padding\" points\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "    \n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, in_channels, channel_list, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        current_in = in_channels\n",
    "        for i, out_ch in enumerate(channel_list):\n",
    "            dilation = 2 ** i\n",
    "            block = TCNBlock(current_in, out_ch, kernel_size, dilation=dilation)\n",
    "            blocks.append(block)\n",
    "            current_in = out_ch\n",
    "        self.network = nn.Sequential(*blocks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class TCNForecastingModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, output_size, kernel_size=2, dropout=0.2, input_window=30):\n",
    "        super(TCNForecastingModel, self).__init__()\n",
    "        self.tcn = TCN(num_inputs, num_channels, kernel_size, dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "        self.input_window = input_window\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = self.tcn(x)\n",
    "        y = y[:, :, -1]\n",
    "        return self.fc(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"day_of_week\"] = data[\"date\"].dt.dayofweek \n",
    "data[\"day_of_month\"] = data[\"date\"].dt.day\n",
    "data[\"month\"] = data[\"date\"].dt.month\n",
    "data[\"day_of_year\"] = data[\"date\"].dt.dayofyear\n",
    "data[\"year\"] = data[\"date\"].dt.year\n",
    "\n",
    "data = pd.get_dummies(data, columns=['most_common_age_bin'], drop_first=True, dtype=int)\n",
    "\n",
    "# Define numerical columns (exclude 'product_group' and 'date')\n",
    "numerical_columns = [\n",
    "    'transaction_count',\n",
    "    'avg_price',\n",
    "    'unique_customers',\n",
    "    'unique_articles_sold',\n",
    "    'median_age',\n",
    "    'fashion_news_subscribers',\n",
    "    'first_purchase_days_ago',\n",
    "    'recent_purchase_days_ago',\n",
    "    'day_of_week',\n",
    "    'day_of_month',\n",
    "    'month',\n",
    "    'day_of_year',\n",
    "    'year'\n",
    "]\n",
    "\n",
    "# categorical_columns = [\n",
    "#     'most_common_age_bin_20-29',\n",
    "#     'most_common_age_bin_30-39',\n",
    "#     'most_common_age_bin_40-49',\n",
    "#     'most_common_age_bin_50-59',\n",
    "#     'most_common_age_bin_60+'\n",
    "# ]\n",
    "\n",
    "# categorical_columns = [\n",
    "#     'most_common_age_bin_20-29', 'most_common_age_bin_30-39',\n",
    "#     'most_common_age_bin_40-49', 'most_common_age_bin_50-59', 'most_common_age_bin_60+'\n",
    "# ]\n",
    "\n",
    "# # One-hot encode the categorical columns\n",
    "# data = pd.get_dummies(data, columns=categorical_columns, dtype=int)\n",
    "\n",
    "data.drop(columns=['std_price', 'club_member_ratio'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_groups = [\n",
    "    \"Bra Black Solid\",\n",
    "    \"Dress Black Solid\",\n",
    "    \"Leggings/Tights Black Solid\",\n",
    "    \"Sweater Black Solid\",\n",
    "    \"T-shirt Black Solid\",\n",
    "    \"T-shirt White Solid\",\n",
    "    \"Top Black Solid\",\n",
    "    \"Trousers Black Solid\",\n",
    "    \"Trousers Blue Denim\",\n",
    "    \"Vest top Black Solid\"\n",
    "]\n",
    "\n",
    "# remove top 10 products from pretrain data\n",
    "pretrain_data = data[~data['product_group'].isin(top_10_groups)].copy()\n",
    "top10_data = data[data['product_group'].isin(top_10_groups)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                         datetime64[ns]\n",
       "product_group                        object\n",
       "transaction_count                     int64\n",
       "avg_price                           float64\n",
       "sales_channel                         int64\n",
       "unique_customers                      int64\n",
       "unique_articles_sold                  int64\n",
       "median_age                          float64\n",
       "fashion_news_subscribers              int64\n",
       "first_purchase_days_ago               int64\n",
       "recent_purchase_days_ago              int64\n",
       "product_type_name                    object\n",
       "colour_group_name                    object\n",
       "graphical_appearance_name            object\n",
       "day_of_week                           int32\n",
       "day_of_month                          int32\n",
       "month                                 int32\n",
       "day_of_year                           int32\n",
       "year                                  int32\n",
       "most_common_age_bin_20-29             int64\n",
       "most_common_age_bin_30-39             int64\n",
       "most_common_age_bin_40-49             int64\n",
       "most_common_age_bin_50-59             int64\n",
       "most_common_age_bin_60+               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Bra Black Solid ===\n",
      "Bra Black Solid | Epoch 10/50, Train Loss: 0.007758, Val Loss: 0.004996\n",
      "Bra Black Solid | Epoch 20/50, Train Loss: 0.007446, Val Loss: 0.005058\n",
      "Bra Black Solid | Epoch 30/50, Train Loss: 0.006672, Val Loss: 0.005185\n",
      "Bra Black Solid | Epoch 40/50, Train Loss: 0.006254, Val Loss: 0.005404\n",
      "Bra Black Solid | Epoch 50/50, Train Loss: 0.005921, Val Loss: 0.005068\n",
      "Bra Black Solid => MAE: 163.9274, RMSE: 200.0572, MAPE: 31.13%, R2: -0.0130\n",
      "\n",
      "=== Processing Dress Black Solid ===\n",
      "Dress Black Solid | Epoch 10/50, Train Loss: 0.013949, Val Loss: 0.018836\n",
      "Dress Black Solid | Epoch 20/50, Train Loss: 0.012199, Val Loss: 0.015234\n",
      "Dress Black Solid | Epoch 30/50, Train Loss: 0.010285, Val Loss: 0.012193\n",
      "Dress Black Solid | Epoch 40/50, Train Loss: 0.009881, Val Loss: 0.010613\n",
      "Dress Black Solid | Epoch 50/50, Train Loss: 0.009586, Val Loss: 0.009749\n",
      "Dress Black Solid => MAE: 257.9106, RMSE: 403.9576, MAPE: 22.39%, R2: 0.4642\n",
      "\n",
      "=== Processing Leggings/Tights Black Solid ===\n",
      "Leggings/Tights Black Solid | Epoch 10/50, Train Loss: 0.007864, Val Loss: 0.002898\n",
      "Leggings/Tights Black Solid | Epoch 20/50, Train Loss: 0.006784, Val Loss: 0.002613\n",
      "Leggings/Tights Black Solid | Epoch 30/50, Train Loss: 0.006122, Val Loss: 0.002311\n",
      "Leggings/Tights Black Solid | Epoch 40/50, Train Loss: 0.005573, Val Loss: 0.002312\n",
      "Leggings/Tights Black Solid | Epoch 50/50, Train Loss: 0.005097, Val Loss: 0.002335\n",
      "Leggings/Tights Black Solid => MAE: 111.5623, RMSE: 134.4484, MAPE: 31.35%, R2: 0.3140\n",
      "\n",
      "=== Processing Sweater Black Solid ===\n",
      "Sweater Black Solid | Epoch 10/50, Train Loss: 0.005787, Val Loss: 0.001583\n",
      "Sweater Black Solid | Epoch 20/50, Train Loss: 0.004367, Val Loss: 0.001364\n",
      "Sweater Black Solid | Epoch 30/50, Train Loss: 0.004192, Val Loss: 0.001379\n",
      "Sweater Black Solid | Epoch 40/50, Train Loss: 0.003803, Val Loss: 0.001237\n",
      "Sweater Black Solid | Epoch 50/50, Train Loss: 0.003230, Val Loss: 0.001102\n",
      "Sweater Black Solid => MAE: 161.8791, RMSE: 218.9981, MAPE: 78.72%, R2: 0.5852\n",
      "\n",
      "=== Processing T-shirt Black Solid ===\n",
      "T-shirt Black Solid | Epoch 10/50, Train Loss: 0.011994, Val Loss: 0.006265\n",
      "T-shirt Black Solid | Epoch 20/50, Train Loss: 0.011368, Val Loss: 0.006476\n",
      "T-shirt Black Solid | Epoch 30/50, Train Loss: 0.009294, Val Loss: 0.006050\n",
      "T-shirt Black Solid | Epoch 40/50, Train Loss: 0.009524, Val Loss: 0.005546\n",
      "T-shirt Black Solid | Epoch 50/50, Train Loss: 0.008633, Val Loss: 0.005573\n",
      "T-shirt Black Solid => MAE: 140.3677, RMSE: 179.4650, MAPE: 21.42%, R2: -0.5513\n",
      "\n",
      "=== Processing T-shirt White Solid ===\n",
      "T-shirt White Solid | Epoch 10/50, Train Loss: 0.010828, Val Loss: 0.004968\n",
      "T-shirt White Solid | Epoch 20/50, Train Loss: 0.007811, Val Loss: 0.003588\n",
      "T-shirt White Solid | Epoch 30/50, Train Loss: 0.008070, Val Loss: 0.003224\n",
      "T-shirt White Solid | Epoch 40/50, Train Loss: 0.006946, Val Loss: 0.002925\n",
      "T-shirt White Solid | Epoch 50/50, Train Loss: 0.005592, Val Loss: 0.002742\n",
      "T-shirt White Solid => MAE: 109.7874, RMSE: 139.7129, MAPE: 22.37%, R2: 0.5193\n",
      "\n",
      "=== Processing Top Black Solid ===\n",
      "Top Black Solid | Epoch 10/50, Train Loss: 0.004925, Val Loss: 0.003231\n",
      "Top Black Solid | Epoch 20/50, Train Loss: 0.004270, Val Loss: 0.002730\n",
      "Top Black Solid | Epoch 30/50, Train Loss: 0.003915, Val Loss: 0.002545\n",
      "Top Black Solid | Epoch 40/50, Train Loss: 0.003700, Val Loss: 0.002454\n",
      "Top Black Solid | Epoch 50/50, Train Loss: 0.003613, Val Loss: 0.002635\n",
      "Top Black Solid => MAE: 157.4124, RMSE: 204.6428, MAPE: 24.04%, R2: 0.0358\n",
      "\n",
      "=== Processing Trousers Black Solid ===\n",
      "Trousers Black Solid | Epoch 10/50, Train Loss: 0.010599, Val Loss: 0.001470\n",
      "Trousers Black Solid | Epoch 20/50, Train Loss: 0.008715, Val Loss: 0.001411\n",
      "Trousers Black Solid | Epoch 30/50, Train Loss: 0.007107, Val Loss: 0.001289\n",
      "Trousers Black Solid | Epoch 40/50, Train Loss: 0.007314, Val Loss: 0.001413\n",
      "Trousers Black Solid | Epoch 50/50, Train Loss: 0.007236, Val Loss: 0.001386\n",
      "Trousers Black Solid => MAE: 203.5538, RMSE: 271.9925, MAPE: 19.97%, R2: 0.0796\n",
      "\n",
      "=== Processing Trousers Blue Denim ===\n",
      "Trousers Blue Denim | Epoch 10/50, Train Loss: 0.012684, Val Loss: 0.003532\n",
      "Trousers Blue Denim | Epoch 20/50, Train Loss: 0.011073, Val Loss: 0.003583\n",
      "Trousers Blue Denim | Epoch 30/50, Train Loss: 0.009573, Val Loss: 0.003361\n",
      "Trousers Blue Denim | Epoch 40/50, Train Loss: 0.009295, Val Loss: 0.003147\n",
      "Trousers Blue Denim | Epoch 50/50, Train Loss: 0.008836, Val Loss: 0.003053\n",
      "Trousers Blue Denim => MAE: 122.4440, RMSE: 164.7276, MAPE: 26.31%, R2: 0.2878\n",
      "\n",
      "=== Processing Vest top Black Solid ===\n",
      "Vest top Black Solid | Epoch 10/50, Train Loss: 0.013195, Val Loss: 0.007586\n",
      "Vest top Black Solid | Epoch 20/50, Train Loss: 0.009995, Val Loss: 0.005731\n",
      "Vest top Black Solid | Epoch 30/50, Train Loss: 0.009283, Val Loss: 0.004767\n",
      "Vest top Black Solid | Epoch 40/50, Train Loss: 0.009239, Val Loss: 0.003841\n",
      "Vest top Black Solid | Epoch 50/50, Train Loss: 0.008446, Val Loss: 0.004101\n",
      "Vest top Black Solid => MAE: 165.7296, RMSE: 211.5958, MAPE: 25.15%, R2: 0.5630\n",
      "\n",
      "All TCN processing completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Example product groups\n",
    "top_10_groups = [\n",
    "    \"Bra Black Solid\",\n",
    "    \"Dress Black Solid\",\n",
    "    \"Leggings/Tights Black Solid\",\n",
    "    \"Sweater Black Solid\",\n",
    "    \"T-shirt Black Solid\",\n",
    "    \"T-shirt White Solid\",\n",
    "    \"Top Black Solid\",\n",
    "    \"Trousers Black Solid\",\n",
    "    \"Trousers Blue Denim\",\n",
    "    \"Vest top Black Solid\"\n",
    "]\n",
    "\n",
    "output_dir = \"final_version/output/google_trends/1_day/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Filter data for this product group\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    if product_data.empty:\n",
    "        print(f\"No data for {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "    product_data.set_index('date', inplace=True)\n",
    "    product_data = product_data.asfreq('D').fillna(0)\n",
    "    \n",
    "    # 2. Identify matched keywords (up to 3)\n",
    "    matched_keywords = []\n",
    "    for col in ['product_type_name', 'colour_group_name', 'graphical_appearance_name']:\n",
    "        if col in product_data.columns:\n",
    "            val = product_data[col].iloc[0]\n",
    "            if val in keywords_list:\n",
    "                matched_keywords.append(val)\n",
    "    \n",
    "    matched_keywords = matched_keywords[:3]  # keep at most 3\n",
    "\n",
    "    # 3. Merge each matched trend\n",
    "    if matched_keywords:\n",
    "        for keyword in matched_keywords:\n",
    "            trend_file = get_trends_file(keyword)\n",
    "            if trend_file:\n",
    "                trend_data = load_and_process_trends(trend_file, product_data)\n",
    "                product_data = add_trends_to_product_data(product_data, trend_data, keyword)\n",
    "            else:\n",
    "                print(f\"Warning: Trend file for {keyword} not found.\")\n",
    "    else:\n",
    "        print(f\"No matched keywords for {product_group}.\")\n",
    "\n",
    "    # 4. Drop unwanted columns\n",
    "    drop_cols = [\n",
    "        'product_group', 'product_type_name', \n",
    "        'colour_group_name', 'graphical_appearance_name',\n",
    "    ]\n",
    "    product_data.drop(columns=[c for c in drop_cols if c in product_data.columns],\n",
    "                      inplace=True, errors='ignore')\n",
    "\n",
    "    if 'transaction_count' not in product_data.columns:\n",
    "        print(f\"No transaction_count in {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 5. Train/Validation Split (80/20)\n",
    "    total_len = len(product_data)\n",
    "    if total_len < 50:\n",
    "        print(f\"Not enough data for {product_group}, skipping.\")\n",
    "        continue\n",
    "    split_idx = int(0.8 * total_len)\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "    \n",
    "    # 6. Scale features & target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = train_df.drop(columns=['transaction_count'])\n",
    "    y_train = train_df[['transaction_count']]\n",
    "\n",
    "    X_val = val_df.drop(columns=['transaction_count'])\n",
    "    y_val = val_df[['transaction_count']]\n",
    "\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "\n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "\n",
    "    # 7. Create TimeSeriesDataset\n",
    "    input_window, output_window = 14, 1\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 8. Build TCN\n",
    "    num_features = X_train.shape[1] + 1  \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=[64, 64],\n",
    "        output_size=1,\n",
    "        kernel_size=3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    # 9. Train TCN\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"{product_group} | Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # 10. Evaluate on Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Invert scaling\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    all_groups_results[product_group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "    print(f\"{product_group} => MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"tcn_trends_summary.csv\"))\n",
    "\n",
    "print(\"\\nAll TCN processing completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 15:56:38,587] A new study created in memory with name: no-name-35d186f0-f1ce-4907-9592-cffc8856245f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Bra Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Bra Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 15:56:44,078] Trial 0 finished with value: 0.004116572113707662 and parameters: {'dropout': 0.1381775997051039, 'lr': 0.00012889277554061477, 'kernel_size': 3, 'num_channels_1': 98, 'num_channels_2': 92}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:56:48,277] Trial 1 finished with value: 0.004512680904008448 and parameters: {'dropout': 0.2635857020509152, 'lr': 1.8068649806516738e-05, 'kernel_size': 4, 'num_channels_1': 102, 'num_channels_2': 63}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:56:52,501] Trial 2 finished with value: 0.005563615495339036 and parameters: {'dropout': 0.31082163759597703, 'lr': 1.662212616092473e-05, 'kernel_size': 3, 'num_channels_1': 75, 'num_channels_2': 66}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:56:56,855] Trial 3 finished with value: 0.005289941001683474 and parameters: {'dropout': 0.10301841534111299, 'lr': 0.0002875747369949596, 'kernel_size': 5, 'num_channels_1': 35, 'num_channels_2': 82}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:03,199] Trial 4 finished with value: 0.0063791560474783186 and parameters: {'dropout': 0.3826083489928387, 'lr': 2.5284612204821035e-05, 'kernel_size': 5, 'num_channels_1': 105, 'num_channels_2': 119}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:07,509] Trial 5 finished with value: 0.006755961012095213 and parameters: {'dropout': 0.13133291031506125, 'lr': 0.00028138251330163513, 'kernel_size': 4, 'num_channels_1': 65, 'num_channels_2': 91}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:12,462] Trial 6 finished with value: 0.005711905099451542 and parameters: {'dropout': 0.3695704601194203, 'lr': 0.00013209032508073904, 'kernel_size': 5, 'num_channels_1': 115, 'num_channels_2': 65}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:17,618] Trial 7 finished with value: 0.004777931468561291 and parameters: {'dropout': 0.11239484832841035, 'lr': 0.0006146857739121846, 'kernel_size': 5, 'num_channels_1': 117, 'num_channels_2': 78}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:22,524] Trial 8 finished with value: 0.00661410135217011 and parameters: {'dropout': 0.4721648251130104, 'lr': 0.000900187453542623, 'kernel_size': 4, 'num_channels_1': 61, 'num_channels_2': 122}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:26,526] Trial 9 finished with value: 0.004748449195176363 and parameters: {'dropout': 0.4950123592554978, 'lr': 0.000490543370049865, 'kernel_size': 4, 'num_channels_1': 69, 'num_channels_2': 81}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:29,868] Trial 10 finished with value: 0.004588608001358807 and parameters: {'dropout': 0.19541474357510646, 'lr': 6.168294721051906e-05, 'kernel_size': 2, 'num_channels_1': 93, 'num_channels_2': 32}. Best is trial 0 with value: 0.004116572113707662.\n",
      "[I 2025-03-05 15:57:34,086] Trial 11 finished with value: 0.0039915748639032245 and parameters: {'dropout': 0.22343477681189938, 'lr': 4.754215992362938e-05, 'kernel_size': 3, 'num_channels_1': 94, 'num_channels_2': 45}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:57:37,909] Trial 12 finished with value: 0.0055424115154892204 and parameters: {'dropout': 0.20547437765131782, 'lr': 6.130585013789339e-05, 'kernel_size': 2, 'num_channels_1': 88, 'num_channels_2': 34}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:57:42,544] Trial 13 finished with value: 0.00462008728645742 and parameters: {'dropout': 0.19005271997533235, 'lr': 0.00010876390892279951, 'kernel_size': 3, 'num_channels_1': 87, 'num_channels_2': 103}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:57:46,912] Trial 14 finished with value: 0.004142797784879804 and parameters: {'dropout': 0.26974200620793654, 'lr': 3.880043037769496e-05, 'kernel_size': 3, 'num_channels_1': 122, 'num_channels_2': 48}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:57:52,460] Trial 15 finished with value: 0.0044900483917444944 and parameters: {'dropout': 0.16370082232480965, 'lr': 0.00020411104289415411, 'kernel_size': 3, 'num_channels_1': 102, 'num_channels_2': 100}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:57:55,119] Trial 16 finished with value: 0.0040629105642437935 and parameters: {'dropout': 0.2381790360306987, 'lr': 4.6991472888852464e-05, 'kernel_size': 2, 'num_channels_1': 51, 'num_channels_2': 49}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:57:57,747] Trial 17 finished with value: 0.006811553100124002 and parameters: {'dropout': 0.240802916758413, 'lr': 4.2937155939556175e-05, 'kernel_size': 2, 'num_channels_1': 48, 'num_channels_2': 48}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:58:00,408] Trial 18 finished with value: 0.006388715258799494 and parameters: {'dropout': 0.3170529907017976, 'lr': 1.222871421033977e-05, 'kernel_size': 2, 'num_channels_1': 46, 'num_channels_2': 49}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:58:02,934] Trial 19 finished with value: 0.005125556886196136 and parameters: {'dropout': 0.35048486262899914, 'lr': 3.155808078403354e-05, 'kernel_size': 2, 'num_channels_1': 54, 'num_channels_2': 41}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:58:06,272] Trial 20 finished with value: 0.004031819081865251 and parameters: {'dropout': 0.22775935138035275, 'lr': 6.847814389197801e-05, 'kernel_size': 2, 'num_channels_1': 79, 'num_channels_2': 56}. Best is trial 11 with value: 0.0039915748639032245.\n",
      "[I 2025-03-05 15:58:09,491] Trial 21 finished with value: 0.0038185611367225645 and parameters: {'dropout': 0.2365255151626952, 'lr': 6.868015411910147e-05, 'kernel_size': 2, 'num_channels_1': 79, 'num_channels_2': 59}. Best is trial 21 with value: 0.0038185611367225645.\n",
      "[I 2025-03-05 15:58:12,697] Trial 22 finished with value: 0.005506479227915406 and parameters: {'dropout': 0.2301131274008683, 'lr': 9.127360270950254e-05, 'kernel_size': 2, 'num_channels_1': 80, 'num_channels_2': 57}. Best is trial 21 with value: 0.0038185611367225645.\n",
      "[I 2025-03-05 15:58:16,014] Trial 23 finished with value: 0.004675564356148243 and parameters: {'dropout': 0.27800476684574155, 'lr': 7.08693463947278e-05, 'kernel_size': 3, 'num_channels_1': 77, 'num_channels_2': 59}. Best is trial 21 with value: 0.0038185611367225645.\n",
      "[I 2025-03-05 15:58:19,584] Trial 24 finished with value: 0.0036882545799016954 and parameters: {'dropout': 0.16859886849662006, 'lr': 0.00016458602555604593, 'kernel_size': 2, 'num_channels_1': 84, 'num_channels_2': 68}. Best is trial 24 with value: 0.0036882545799016954.\n",
      "[I 2025-03-05 15:58:23,451] Trial 25 finished with value: 0.0047391606494784355 and parameters: {'dropout': 0.17579640092294346, 'lr': 0.00017542936784490824, 'kernel_size': 3, 'num_channels_1': 86, 'num_channels_2': 75}. Best is trial 24 with value: 0.0036882545799016954.\n",
      "[I 2025-03-05 15:58:27,332] Trial 26 finished with value: 0.0033676255960017444 and parameters: {'dropout': 0.15346383800157667, 'lr': 0.0001660054939198859, 'kernel_size': 2, 'num_channels_1': 111, 'num_channels_2': 70}. Best is trial 26 with value: 0.0033676255960017444.\n",
      "[I 2025-03-05 15:58:31,312] Trial 27 finished with value: 0.003093981696292758 and parameters: {'dropout': 0.1552010073039545, 'lr': 0.00034338723589843453, 'kernel_size': 2, 'num_channels_1': 107, 'num_channels_2': 76}. Best is trial 27 with value: 0.003093981696292758.\n",
      "[I 2025-03-05 15:58:36,060] Trial 28 finished with value: 0.0034321144688874484 and parameters: {'dropout': 0.15629893607273393, 'lr': 0.00038268289060343434, 'kernel_size': 2, 'num_channels_1': 127, 'num_channels_2': 72}. Best is trial 27 with value: 0.003093981696292758.\n",
      "[I 2025-03-05 15:58:42,313] Trial 29 finished with value: 0.0022819156642071904 and parameters: {'dropout': 0.13721910416127028, 'lr': 0.0002926406669070098, 'kernel_size': 2, 'num_channels_1': 127, 'num_channels_2': 90}. Best is trial 29 with value: 0.0022819156642071904.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 15:58:42,354] A new study created in memory with name: no-name-288296fe-8fdc-4082-bcca-eecaf0f586f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Bra Black Solid: {'dropout': 0.13721910416127028, 'lr': 0.0002926406669070098, 'kernel_size': 2, 'num_channels_1': 127, 'num_channels_2': 90}\n",
      "\n",
      "=== Processing Dress Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Dress Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 15:58:46,772] Trial 0 finished with value: 0.023632198106497526 and parameters: {'dropout': 0.1738988734635309, 'lr': 1.1888405206028547e-05, 'kernel_size': 2, 'num_channels_1': 103, 'num_channels_2': 93}. Best is trial 0 with value: 0.023632198106497526.\n",
      "[I 2025-03-05 15:58:51,073] Trial 1 finished with value: 0.01601804499514401 and parameters: {'dropout': 0.21001817999945752, 'lr': 7.79317838468787e-05, 'kernel_size': 2, 'num_channels_1': 65, 'num_channels_2': 70}. Best is trial 1 with value: 0.01601804499514401.\n",
      "[I 2025-03-05 15:58:54,767] Trial 2 finished with value: 0.014124898007139564 and parameters: {'dropout': 0.4312713501331612, 'lr': 0.0001593311902929538, 'kernel_size': 3, 'num_channels_1': 46, 'num_channels_2': 94}. Best is trial 2 with value: 0.014124898007139564.\n",
      "[I 2025-03-05 15:58:58,531] Trial 3 finished with value: 0.019917412381619214 and parameters: {'dropout': 0.14217884084879248, 'lr': 2.9071894325527927e-05, 'kernel_size': 4, 'num_channels_1': 82, 'num_channels_2': 65}. Best is trial 2 with value: 0.014124898007139564.\n",
      "[I 2025-03-05 15:59:02,564] Trial 4 finished with value: 0.008582364046014845 and parameters: {'dropout': 0.2835213699381559, 'lr': 0.0006539548632244381, 'kernel_size': 3, 'num_channels_1': 56, 'num_channels_2': 99}. Best is trial 4 with value: 0.008582364046014845.\n",
      "[I 2025-03-05 15:59:07,237] Trial 5 finished with value: 0.010891966917552055 and parameters: {'dropout': 0.15282543294257264, 'lr': 0.0006634233068067136, 'kernel_size': 4, 'num_channels_1': 65, 'num_channels_2': 109}. Best is trial 4 with value: 0.008582364046014845.\n",
      "[I 2025-03-05 15:59:11,457] Trial 6 finished with value: 0.008626055321656168 and parameters: {'dropout': 0.3559073637909671, 'lr': 0.00031659417045729293, 'kernel_size': 3, 'num_channels_1': 91, 'num_channels_2': 72}. Best is trial 4 with value: 0.008582364046014845.\n",
      "[I 2025-03-05 15:59:17,240] Trial 7 finished with value: 0.010931079636793583 and parameters: {'dropout': 0.47709863100585526, 'lr': 0.00018379442940233892, 'kernel_size': 5, 'num_channels_1': 110, 'num_channels_2': 81}. Best is trial 4 with value: 0.008582364046014845.\n",
      "[I 2025-03-05 15:59:24,493] Trial 8 finished with value: 0.01599671896547079 and parameters: {'dropout': 0.4520302596211101, 'lr': 2.47514164882535e-05, 'kernel_size': 5, 'num_channels_1': 126, 'num_channels_2': 107}. Best is trial 4 with value: 0.008582364046014845.\n",
      "[I 2025-03-05 15:59:30,853] Trial 9 finished with value: 0.014221448311582207 and parameters: {'dropout': 0.19897201736629366, 'lr': 4.063279660874124e-05, 'kernel_size': 5, 'num_channels_1': 120, 'num_channels_2': 127}. Best is trial 4 with value: 0.008582364046014845.\n",
      "[I 2025-03-05 15:59:33,237] Trial 10 finished with value: 0.0076609308598563075 and parameters: {'dropout': 0.28785742634145167, 'lr': 0.0009508080708024445, 'kernel_size': 3, 'num_channels_1': 34, 'num_channels_2': 35}. Best is trial 10 with value: 0.0076609308598563075.\n",
      "[I 2025-03-05 15:59:36,033] Trial 11 finished with value: 0.007601068343501538 and parameters: {'dropout': 0.2990607957008566, 'lr': 0.0009623514951648911, 'kernel_size': 3, 'num_channels_1': 36, 'num_channels_2': 34}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:38,437] Trial 12 finished with value: 0.009082494210451842 and parameters: {'dropout': 0.2935336005890701, 'lr': 0.0007244348483092548, 'kernel_size': 3, 'num_channels_1': 35, 'num_channels_2': 37}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:40,630] Trial 13 finished with value: 0.008440595958381892 and parameters: {'dropout': 0.36254794665488393, 'lr': 0.000921337415045102, 'kernel_size': 2, 'num_channels_1': 34, 'num_channels_2': 35}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:43,492] Trial 14 finished with value: 0.009991021757014096 and parameters: {'dropout': 0.2372761100694327, 'lr': 0.00038873595827197113, 'kernel_size': 4, 'num_channels_1': 47, 'num_channels_2': 47}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:46,252] Trial 15 finished with value: 0.009238159062806516 and parameters: {'dropout': 0.36135934011090765, 'lr': 0.0003528187726600419, 'kernel_size': 3, 'num_channels_1': 44, 'num_channels_2': 54}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:50,109] Trial 16 finished with value: 0.011505910498090088 and parameters: {'dropout': 0.2647381783694079, 'lr': 0.000188660239584984, 'kernel_size': 4, 'num_channels_1': 67, 'num_channels_2': 52}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:52,434] Trial 17 finished with value: 0.02219906395766884 and parameters: {'dropout': 0.3310039395749692, 'lr': 8.642925781865921e-05, 'kernel_size': 2, 'num_channels_1': 33, 'num_channels_2': 33}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:55,296] Trial 18 finished with value: 0.008481262030545621 and parameters: {'dropout': 0.10435390398295658, 'lr': 0.00045450298213284203, 'kernel_size': 3, 'num_channels_1': 54, 'num_channels_2': 44}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 15:59:58,905] Trial 19 finished with value: 0.008816666202619672 and parameters: {'dropout': 0.39957996165114595, 'lr': 0.000989724514024737, 'kernel_size': 4, 'num_channels_1': 77, 'num_channels_2': 56}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:01,640] Trial 20 finished with value: 0.009660026314668358 and parameters: {'dropout': 0.3167540016729853, 'lr': 0.00027176907730594603, 'kernel_size': 2, 'num_channels_1': 56, 'num_channels_2': 44}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:03,781] Trial 21 finished with value: 0.01036983773810789 and parameters: {'dropout': 0.38863339438116684, 'lr': 0.0009952695399064605, 'kernel_size': 2, 'num_channels_1': 33, 'num_channels_2': 34}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:06,376] Trial 22 finished with value: 0.009715850022621453 and parameters: {'dropout': 0.25517938475736124, 'lr': 0.0005422763646137034, 'kernel_size': 2, 'num_channels_1': 41, 'num_channels_2': 40}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:10,005] Trial 23 finished with value: 0.009405800001695752 and parameters: {'dropout': 0.3384844515238141, 'lr': 0.0009501412738292648, 'kernel_size': 3, 'num_channels_1': 39, 'num_channels_2': 62}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:12,612] Trial 24 finished with value: 0.008903182880021631 and parameters: {'dropout': 0.30571747074699723, 'lr': 0.0005350082054970627, 'kernel_size': 3, 'num_channels_1': 51, 'num_channels_2': 32}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:15,135] Trial 25 finished with value: 0.013532447163015604 and parameters: {'dropout': 0.3887025819158936, 'lr': 0.00024335160125789815, 'kernel_size': 2, 'num_channels_1': 32, 'num_channels_2': 47}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:18,735] Trial 26 finished with value: 0.010151376482099295 and parameters: {'dropout': 0.21835202175294627, 'lr': 0.0007131092042208628, 'kernel_size': 3, 'num_channels_1': 40, 'num_channels_2': 80}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:21,536] Trial 27 finished with value: 0.014986979309469461 and parameters: {'dropout': 0.2666743443770121, 'lr': 0.00013309809597014964, 'kernel_size': 2, 'num_channels_1': 61, 'num_channels_2': 41}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:25,221] Trial 28 finished with value: 0.0198336788918823 and parameters: {'dropout': 0.36195452309332143, 'lr': 6.299097903941875e-05, 'kernel_size': 3, 'num_channels_1': 75, 'num_channels_2': 59}. Best is trial 11 with value: 0.007601068343501538.\n",
      "[I 2025-03-05 16:00:28,522] Trial 29 finished with value: 0.02601689803414047 and parameters: {'dropout': 0.42606437251392615, 'lr': 1.3000079408659445e-05, 'kernel_size': 2, 'num_channels_1': 96, 'num_channels_2': 50}. Best is trial 11 with value: 0.007601068343501538.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:00:28,579] A new study created in memory with name: no-name-ce759f49-f122-4587-8b3d-d5ee8c94963e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Dress Black Solid: {'dropout': 0.2990607957008566, 'lr': 0.0009623514951648911, 'kernel_size': 3, 'num_channels_1': 36, 'num_channels_2': 34}\n",
      "\n",
      "=== Processing Leggings/Tights Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Leggings/Tights Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:00:32,208] Trial 0 finished with value: 0.0033781217178329827 and parameters: {'dropout': 0.3380621402688181, 'lr': 0.00013149070436603363, 'kernel_size': 5, 'num_channels_1': 95, 'num_channels_2': 32}. Best is trial 0 with value: 0.0033781217178329827.\n",
      "[I 2025-03-05 16:00:35,277] Trial 1 finished with value: 0.0034444604301825167 and parameters: {'dropout': 0.49881020120828024, 'lr': 0.0005131244737393343, 'kernel_size': 4, 'num_channels_1': 38, 'num_channels_2': 57}. Best is trial 0 with value: 0.0033781217178329827.\n",
      "[I 2025-03-05 16:00:38,730] Trial 2 finished with value: 0.003396551945479587 and parameters: {'dropout': 0.22004879339224145, 'lr': 0.00015618763935102992, 'kernel_size': 3, 'num_channels_1': 127, 'num_channels_2': 43}. Best is trial 0 with value: 0.0033781217178329827.\n",
      "[I 2025-03-05 16:00:43,648] Trial 3 finished with value: 0.0035388207295909526 and parameters: {'dropout': 0.18679584043860933, 'lr': 4.962895278880783e-05, 'kernel_size': 5, 'num_channels_1': 96, 'num_channels_2': 87}. Best is trial 0 with value: 0.0033781217178329827.\n",
      "[I 2025-03-05 16:00:48,239] Trial 4 finished with value: 0.00225805074442178 and parameters: {'dropout': 0.14514785629970556, 'lr': 0.0004343334314199055, 'kernel_size': 2, 'num_channels_1': 128, 'num_channels_2': 98}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:00:51,875] Trial 5 finished with value: 0.0040022670989856126 and parameters: {'dropout': 0.10964325181151069, 'lr': 1.731215900723552e-05, 'kernel_size': 3, 'num_channels_1': 113, 'num_channels_2': 42}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:00:54,671] Trial 6 finished with value: 0.0039019132498651745 and parameters: {'dropout': 0.3796014460694037, 'lr': 0.00015750983774751764, 'kernel_size': 2, 'num_channels_1': 34, 'num_channels_2': 40}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:00:58,902] Trial 7 finished with value: 0.0029418942984193565 and parameters: {'dropout': 0.48761566632927544, 'lr': 4.6421310220621206e-05, 'kernel_size': 3, 'num_channels_1': 60, 'num_channels_2': 77}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:01:05,284] Trial 8 finished with value: 0.003427356155589223 and parameters: {'dropout': 0.2204220783685089, 'lr': 0.00013100424089705376, 'kernel_size': 4, 'num_channels_1': 73, 'num_channels_2': 77}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:01:12,046] Trial 9 finished with value: 0.0035660231951624155 and parameters: {'dropout': 0.3516591210971538, 'lr': 0.0008977423072672665, 'kernel_size': 5, 'num_channels_1': 122, 'num_channels_2': 75}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:01:17,160] Trial 10 finished with value: 0.0025636044127168136 and parameters: {'dropout': 0.10510097266848395, 'lr': 0.00037990472517325524, 'kernel_size': 2, 'num_channels_1': 103, 'num_channels_2': 121}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:01:22,662] Trial 11 finished with value: 0.0028221344109624624 and parameters: {'dropout': 0.10906487741210363, 'lr': 0.0004744579246875366, 'kernel_size': 2, 'num_channels_1': 103, 'num_channels_2': 122}. Best is trial 4 with value: 0.00225805074442178.\n",
      "[I 2025-03-05 16:01:28,536] Trial 12 finished with value: 0.0021537668304517864 and parameters: {'dropout': 0.16304284199680225, 'lr': 0.00034392787901308525, 'kernel_size': 2, 'num_channels_1': 111, 'num_channels_2': 120}. Best is trial 12 with value: 0.0021537668304517864.\n",
      "[I 2025-03-05 16:01:35,077] Trial 13 finished with value: 0.0018853018758818508 and parameters: {'dropout': 0.17054121666588165, 'lr': 0.0002787730427122464, 'kernel_size': 2, 'num_channels_1': 116, 'num_channels_2': 105}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:01:39,996] Trial 14 finished with value: 0.0022811583417933433 and parameters: {'dropout': 0.2752316060864695, 'lr': 0.0002487218912380011, 'kernel_size': 3, 'num_channels_1': 81, 'num_channels_2': 104}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:01:45,848] Trial 15 finished with value: 0.002681076689623296 and parameters: {'dropout': 0.25492462823234635, 'lr': 0.0009455468731311047, 'kernel_size': 2, 'num_channels_1': 114, 'num_channels_2': 109}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:01:52,086] Trial 16 finished with value: 0.0033783896826207636 and parameters: {'dropout': 0.1679818397507749, 'lr': 5.671467260358693e-05, 'kernel_size': 2, 'num_channels_1': 87, 'num_channels_2': 128}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:01:58,969] Trial 17 finished with value: 0.003955843066796661 and parameters: {'dropout': 0.20987739292799884, 'lr': 0.0002549556159946293, 'kernel_size': 4, 'num_channels_1': 115, 'num_channels_2': 110}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:03,860] Trial 18 finished with value: 0.003195420280098915 and parameters: {'dropout': 0.2979444907924057, 'lr': 7.346201734081716e-05, 'kernel_size': 3, 'num_channels_1': 66, 'num_channels_2': 91}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:09,074] Trial 19 finished with value: 0.00411979672499001 and parameters: {'dropout': 0.1499734240237604, 'lr': 2.6767126098007717e-05, 'kernel_size': 2, 'num_channels_1': 51, 'num_channels_2': 115}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:12,956] Trial 20 finished with value: 0.0028693902539089324 and parameters: {'dropout': 0.4378209329080597, 'lr': 0.0002529927297759717, 'kernel_size': 3, 'num_channels_1': 105, 'num_channels_2': 64}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:17,685] Trial 21 finished with value: 0.0026278129778802397 and parameters: {'dropout': 0.14968085586816018, 'lr': 0.0005995996913511181, 'kernel_size': 2, 'num_channels_1': 127, 'num_channels_2': 96}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:22,721] Trial 22 finished with value: 0.002149762533372268 and parameters: {'dropout': 0.14534211874398362, 'lr': 0.00033686196900825665, 'kernel_size': 2, 'num_channels_1': 119, 'num_channels_2': 102}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:28,378] Trial 23 finished with value: 0.002210195083171129 and parameters: {'dropout': 0.24464653960599403, 'lr': 0.0003037180757688597, 'kernel_size': 2, 'num_channels_1': 117, 'num_channels_2': 103}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:34,958] Trial 24 finished with value: 0.0027786513324826955 and parameters: {'dropout': 0.19404654118777293, 'lr': 0.0006271465206883695, 'kernel_size': 2, 'num_channels_1': 108, 'num_channels_2': 113}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:40,318] Trial 25 finished with value: 0.002767648210283369 and parameters: {'dropout': 0.16895938790499326, 'lr': 0.00018679341261465453, 'kernel_size': 3, 'num_channels_1': 93, 'num_channels_2': 87}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:45,555] Trial 26 finished with value: 0.0023341093212366102 and parameters: {'dropout': 0.13061032751653645, 'lr': 0.00033929344618102215, 'kernel_size': 2, 'num_channels_1': 119, 'num_channels_2': 128}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:51,126] Trial 27 finished with value: 0.00407315245247446 and parameters: {'dropout': 0.24455555662628692, 'lr': 8.515100894696135e-05, 'kernel_size': 3, 'num_channels_1': 111, 'num_channels_2': 121}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:02:56,363] Trial 28 finished with value: 0.002825482966727577 and parameters: {'dropout': 0.18018937435099774, 'lr': 0.00020327246428128762, 'kernel_size': 2, 'num_channels_1': 101, 'num_channels_2': 103}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "[I 2025-03-05 16:03:01,607] Trial 29 finished with value: 0.0024311857763677836 and parameters: {'dropout': 0.3445146389628234, 'lr': 0.00011868246505221923, 'kernel_size': 4, 'num_channels_1': 89, 'num_channels_2': 95}. Best is trial 13 with value: 0.0018853018758818508.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:03:01,645] A new study created in memory with name: no-name-0127615b-dc3b-407c-9d6e-e0a36d576f05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Leggings/Tights Black Solid: {'dropout': 0.17054121666588165, 'lr': 0.0002787730427122464, 'kernel_size': 2, 'num_channels_1': 116, 'num_channels_2': 105}\n",
      "\n",
      "=== Processing Sweater Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Sweater Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:03:05,565] Trial 0 finished with value: 0.0032926193438470364 and parameters: {'dropout': 0.481790176387518, 'lr': 1.4665349603016886e-05, 'kernel_size': 3, 'num_channels_1': 118, 'num_channels_2': 63}. Best is trial 0 with value: 0.0032926193438470364.\n",
      "[I 2025-03-05 16:03:10,647] Trial 1 finished with value: 0.0017350461333990098 and parameters: {'dropout': 0.34644154237448344, 'lr': 2.031230021156042e-05, 'kernel_size': 3, 'num_channels_1': 117, 'num_channels_2': 87}. Best is trial 1 with value: 0.0017350461333990098.\n",
      "[I 2025-03-05 16:03:16,333] Trial 2 finished with value: 0.0012186000793008133 and parameters: {'dropout': 0.23860330054475454, 'lr': 0.0004303226895870442, 'kernel_size': 3, 'num_channels_1': 113, 'num_channels_2': 112}. Best is trial 2 with value: 0.0012186000793008133.\n",
      "[I 2025-03-05 16:03:20,785] Trial 3 finished with value: 0.0021124499384313823 and parameters: {'dropout': 0.1857651432747381, 'lr': 2.3998686600763904e-05, 'kernel_size': 5, 'num_channels_1': 61, 'num_channels_2': 34}. Best is trial 2 with value: 0.0012186000793008133.\n",
      "[I 2025-03-05 16:03:25,587] Trial 4 finished with value: 0.0010128069843631238 and parameters: {'dropout': 0.1314122811305224, 'lr': 8.394545551552424e-05, 'kernel_size': 5, 'num_channels_1': 64, 'num_channels_2': 91}. Best is trial 4 with value: 0.0010128069843631238.\n",
      "[I 2025-03-05 16:03:29,767] Trial 5 finished with value: 0.0025665371678769587 and parameters: {'dropout': 0.33873451243790986, 'lr': 2.4250500269080637e-05, 'kernel_size': 3, 'num_channels_1': 101, 'num_channels_2': 73}. Best is trial 4 with value: 0.0010128069843631238.\n",
      "[I 2025-03-05 16:03:33,112] Trial 6 finished with value: 0.0011157352419104427 and parameters: {'dropout': 0.3851313519795089, 'lr': 0.00013588592528282126, 'kernel_size': 3, 'num_channels_1': 57, 'num_channels_2': 55}. Best is trial 4 with value: 0.0010128069843631238.\n",
      "[I 2025-03-05 16:03:37,627] Trial 7 finished with value: 0.0012263294425792991 and parameters: {'dropout': 0.1438319065769302, 'lr': 7.83604020901627e-05, 'kernel_size': 5, 'num_channels_1': 97, 'num_channels_2': 56}. Best is trial 4 with value: 0.0010128069843631238.\n",
      "[I 2025-03-05 16:03:40,748] Trial 8 finished with value: 0.003034782424219884 and parameters: {'dropout': 0.26617713871650384, 'lr': 3.606039654957865e-05, 'kernel_size': 3, 'num_channels_1': 62, 'num_channels_2': 55}. Best is trial 4 with value: 0.0010128069843631238.\n",
      "[I 2025-03-05 16:03:44,410] Trial 9 finished with value: 0.0009318925149273127 and parameters: {'dropout': 0.20521962406706293, 'lr': 0.0007814454856165734, 'kernel_size': 3, 'num_channels_1': 61, 'num_channels_2': 79}. Best is trial 9 with value: 0.0009318925149273127.\n",
      "[I 2025-03-05 16:03:49,571] Trial 10 finished with value: 0.0007245192944537848 and parameters: {'dropout': 0.20113070876014022, 'lr': 0.000946695960257369, 'kernel_size': 2, 'num_channels_1': 35, 'num_channels_2': 128}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:03:54,584] Trial 11 finished with value: 0.001010563626186922 and parameters: {'dropout': 0.21005967600273281, 'lr': 0.0009626871076484537, 'kernel_size': 2, 'num_channels_1': 35, 'num_channels_2': 126}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:03:58,858] Trial 12 finished with value: 0.0007585141400340944 and parameters: {'dropout': 0.18196408263491493, 'lr': 0.0009607158257907606, 'kernel_size': 2, 'num_channels_1': 32, 'num_channels_2': 103}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:04:03,639] Trial 13 finished with value: 0.001578474702546373 and parameters: {'dropout': 0.11813578314821699, 'lr': 0.00028779407809710345, 'kernel_size': 2, 'num_channels_1': 32, 'num_channels_2': 107}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:04:08,517] Trial 14 finished with value: 0.0008809966238914058 and parameters: {'dropout': 0.2792450403660143, 'lr': 0.00039467576569384127, 'kernel_size': 2, 'num_channels_1': 45, 'num_channels_2': 128}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:04:14,971] Trial 15 finished with value: 0.0008697362907696515 and parameters: {'dropout': 0.16261578094235912, 'lr': 0.00020699854163313056, 'kernel_size': 4, 'num_channels_1': 81, 'num_channels_2': 106}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:04:20,415] Trial 16 finished with value: 0.000844120784313418 and parameters: {'dropout': 0.23448267656869412, 'lr': 0.0005686650814428513, 'kernel_size': 2, 'num_channels_1': 43, 'num_channels_2': 116}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:04:26,041] Trial 17 finished with value: 0.000988127413438633 and parameters: {'dropout': 0.323860179083919, 'lr': 0.0002287790048522442, 'kernel_size': 4, 'num_channels_1': 81, 'num_channels_2': 96}. Best is trial 10 with value: 0.0007245192944537848.\n",
      "[I 2025-03-05 16:04:30,237] Trial 18 finished with value: 0.0006824048701673746 and parameters: {'dropout': 0.10603756895045297, 'lr': 0.0009491984473514982, 'kernel_size': 2, 'num_channels_1': 49, 'num_channels_2': 99}. Best is trial 18 with value: 0.0006824048701673746.\n",
      "[I 2025-03-05 16:04:35,624] Trial 19 finished with value: 0.0013606103137135505 and parameters: {'dropout': 0.10285508450431236, 'lr': 5.112898723385564e-05, 'kernel_size': 4, 'num_channels_1': 47, 'num_channels_2': 119}. Best is trial 18 with value: 0.0006824048701673746.\n",
      "[I 2025-03-05 16:04:40,658] Trial 20 finished with value: 0.0006457043607952073 and parameters: {'dropout': 0.10141297716503167, 'lr': 0.0006024493223808809, 'kernel_size': 2, 'num_channels_1': 74, 'num_channels_2': 97}. Best is trial 20 with value: 0.0006457043607952073.\n",
      "[I 2025-03-05 16:04:45,148] Trial 21 finished with value: 0.0008105181565042585 and parameters: {'dropout': 0.10672904630422161, 'lr': 0.0005770286995558885, 'kernel_size': 2, 'num_channels_1': 73, 'num_channels_2': 97}. Best is trial 20 with value: 0.0006457043607952073.\n",
      "[I 2025-03-05 16:04:49,705] Trial 22 finished with value: 0.0005188932613236829 and parameters: {'dropout': 0.15808145024214382, 'lr': 0.0005935354185147409, 'kernel_size': 2, 'num_channels_1': 48, 'num_channels_2': 82}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:04:53,866] Trial 23 finished with value: 0.0008697645622305572 and parameters: {'dropout': 0.15416035041421017, 'lr': 0.0005319582443469874, 'kernel_size': 2, 'num_channels_1': 51, 'num_channels_2': 82}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:04:57,942] Trial 24 finished with value: 0.000637542262848001 and parameters: {'dropout': 0.10045739968133718, 'lr': 0.00031157418068033306, 'kernel_size': 2, 'num_channels_1': 91, 'num_channels_2': 73}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:05:01,747] Trial 25 finished with value: 0.0009790638607228175 and parameters: {'dropout': 0.1568125833371872, 'lr': 0.00013775376515773237, 'kernel_size': 2, 'num_channels_1': 91, 'num_channels_2': 71}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:05:05,074] Trial 26 finished with value: 0.0012057864398229868 and parameters: {'dropout': 0.4519504426776674, 'lr': 0.0003162327421578835, 'kernel_size': 2, 'num_channels_1': 71, 'num_channels_2': 70}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:05:09,069] Trial 27 finished with value: 0.0008319561195094139 and parameters: {'dropout': 0.14219045423895926, 'lr': 0.00014587016891415736, 'kernel_size': 3, 'num_channels_1': 88, 'num_channels_2': 86}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:05:14,174] Trial 28 finished with value: 0.0009434267965843901 and parameters: {'dropout': 0.17010771048993306, 'lr': 0.00020196457016932113, 'kernel_size': 2, 'num_channels_1': 127, 'num_channels_2': 77}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "[I 2025-03-05 16:05:18,272] Trial 29 finished with value: 0.000600570798269473 and parameters: {'dropout': 0.41153010359861886, 'lr': 0.0003798649706214157, 'kernel_size': 3, 'num_channels_1': 71, 'num_channels_2': 66}. Best is trial 22 with value: 0.0005188932613236829.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:05:18,325] A new study created in memory with name: no-name-54cae07a-8831-40ad-8df2-0670fa6e9df0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Sweater Black Solid: {'dropout': 0.15808145024214382, 'lr': 0.0005935354185147409, 'kernel_size': 2, 'num_channels_1': 48, 'num_channels_2': 82}\n",
      "\n",
      "=== Processing T-shirt Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for T-shirt Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:05:22,663] Trial 0 finished with value: 0.008976623840862886 and parameters: {'dropout': 0.36377871098806414, 'lr': 1.7032665634613552e-05, 'kernel_size': 5, 'num_channels_1': 59, 'num_channels_2': 62}. Best is trial 0 with value: 0.008976623840862886.\n",
      "[I 2025-03-05 16:05:28,908] Trial 1 finished with value: 0.004140580305829644 and parameters: {'dropout': 0.14833480910076965, 'lr': 3.94916188267716e-05, 'kernel_size': 5, 'num_channels_1': 98, 'num_channels_2': 111}. Best is trial 1 with value: 0.004140580305829644.\n",
      "[I 2025-03-05 16:05:33,973] Trial 2 finished with value: 0.0059129650646355 and parameters: {'dropout': 0.292276383321343, 'lr': 3.1494612619812234e-05, 'kernel_size': 5, 'num_channels_1': 48, 'num_channels_2': 105}. Best is trial 1 with value: 0.004140580305829644.\n",
      "[I 2025-03-05 16:05:38,015] Trial 3 finished with value: 0.004446345940232277 and parameters: {'dropout': 0.32229836753661945, 'lr': 0.0005544355326553622, 'kernel_size': 4, 'num_channels_1': 94, 'num_channels_2': 39}. Best is trial 1 with value: 0.004140580305829644.\n",
      "[I 2025-03-05 16:05:42,203] Trial 4 finished with value: 0.0032826566835865378 and parameters: {'dropout': 0.10496665587160137, 'lr': 0.0005887969769196279, 'kernel_size': 5, 'num_channels_1': 60, 'num_channels_2': 51}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:05:45,737] Trial 5 finished with value: 0.005340011947555468 and parameters: {'dropout': 0.40387071797194773, 'lr': 2.8363631361429754e-05, 'kernel_size': 3, 'num_channels_1': 58, 'num_channels_2': 58}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:05:50,046] Trial 6 finished with value: 0.004439400129194837 and parameters: {'dropout': 0.29682864108427043, 'lr': 0.0001468693936840542, 'kernel_size': 3, 'num_channels_1': 46, 'num_channels_2': 86}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:05:52,803] Trial 7 finished with value: 0.007075132493628189 and parameters: {'dropout': 0.24310450095708963, 'lr': 4.581574633077996e-05, 'kernel_size': 2, 'num_channels_1': 43, 'num_channels_2': 59}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:05:56,885] Trial 8 finished with value: 0.004927273269277066 and parameters: {'dropout': 0.42790882868767754, 'lr': 5.1091648150975886e-05, 'kernel_size': 3, 'num_channels_1': 62, 'num_channels_2': 100}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:00,553] Trial 9 finished with value: 0.003305490571074188 and parameters: {'dropout': 0.3927340428803815, 'lr': 0.0002518796420502126, 'kernel_size': 3, 'num_channels_1': 116, 'num_channels_2': 50}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:05,796] Trial 10 finished with value: 0.003346391109516844 and parameters: {'dropout': 0.11637708794909692, 'lr': 0.0009345513335490719, 'kernel_size': 4, 'num_channels_1': 78, 'num_channels_2': 128}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:08,677] Trial 11 finished with value: 0.00859581707045436 and parameters: {'dropout': 0.4949206276795263, 'lr': 0.00028866310065790317, 'kernel_size': 2, 'num_channels_1': 113, 'num_channels_2': 32}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:12,542] Trial 12 finished with value: 0.0033045694348402322 and parameters: {'dropout': 0.20342697614317665, 'lr': 0.00023719191203394458, 'kernel_size': 4, 'num_channels_1': 115, 'num_channels_2': 48}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:16,470] Trial 13 finished with value: 0.0055603681015782055 and parameters: {'dropout': 0.18622321337887543, 'lr': 0.00040185138017625354, 'kernel_size': 4, 'num_channels_1': 79, 'num_channels_2': 73}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:20,841] Trial 14 finished with value: 0.004061661381274462 and parameters: {'dropout': 0.20406652190876748, 'lr': 0.00012018470128289902, 'kernel_size': 5, 'num_channels_1': 122, 'num_channels_2': 44}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:26,288] Trial 15 finished with value: 0.004220462823286653 and parameters: {'dropout': 0.1015396602227406, 'lr': 0.0009736796591897392, 'kernel_size': 4, 'num_channels_1': 97, 'num_channels_2': 74}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:31,152] Trial 16 finished with value: 0.004214092378970235 and parameters: {'dropout': 0.17340874560131603, 'lr': 0.00018869153730860874, 'kernel_size': 5, 'num_channels_1': 71, 'num_channels_2': 88}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:34,752] Trial 17 finished with value: 0.003652903810143471 and parameters: {'dropout': 0.22878234473608072, 'lr': 0.0004967752563345683, 'kernel_size': 4, 'num_channels_1': 90, 'num_channels_2': 51}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:39,545] Trial 18 finished with value: 0.004116326058283448 and parameters: {'dropout': 0.1471291439250355, 'lr': 8.725257722499776e-05, 'kernel_size': 5, 'num_channels_1': 108, 'num_channels_2': 70}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:42,593] Trial 19 finished with value: 0.00445444974466227 and parameters: {'dropout': 0.21956983086955936, 'lr': 0.00033941270523838167, 'kernel_size': 4, 'num_channels_1': 68, 'num_channels_2': 42}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:45,020] Trial 20 finished with value: 0.003671385161578655 and parameters: {'dropout': 0.24946072985026907, 'lr': 0.0006483363163775249, 'kernel_size': 5, 'num_channels_1': 32, 'num_channels_2': 34}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:48,516] Trial 21 finished with value: 0.004657923756167292 and parameters: {'dropout': 0.46631062531859563, 'lr': 0.0002412467197389347, 'kernel_size': 3, 'num_channels_1': 123, 'num_channels_2': 51}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:51,964] Trial 22 finished with value: 0.005890178773552179 and parameters: {'dropout': 0.3514392273460079, 'lr': 7.569374899263409e-05, 'kernel_size': 3, 'num_channels_1': 112, 'num_channels_2': 52}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:55,401] Trial 23 finished with value: 0.005904434062540531 and parameters: {'dropout': 0.26185447438786835, 'lr': 0.00019871123075455632, 'kernel_size': 2, 'num_channels_1': 87, 'num_channels_2': 65}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:06:59,201] Trial 24 finished with value: 0.0034745948098134248 and parameters: {'dropout': 0.13770947863367752, 'lr': 0.0003608704945045465, 'kernel_size': 3, 'num_channels_1': 128, 'num_channels_2': 47}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:07:04,220] Trial 25 finished with value: 0.005869601084850728 and parameters: {'dropout': 0.4036660351719879, 'lr': 0.0007200148590552715, 'kernel_size': 4, 'num_channels_1': 104, 'num_channels_2': 81}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:07:08,311] Trial 26 finished with value: 0.003803786030039191 and parameters: {'dropout': 0.17732082482747658, 'lr': 0.00014344569546073912, 'kernel_size': 3, 'num_channels_1': 119, 'num_channels_2': 57}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:07:11,699] Trial 27 finished with value: 0.0036591302370652555 and parameters: {'dropout': 0.3516901782751405, 'lr': 0.0002312296165955475, 'kernel_size': 4, 'num_channels_1': 85, 'num_channels_2': 39}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:07:15,763] Trial 28 finished with value: 0.004647593689151108 and parameters: {'dropout': 0.26714454753797057, 'lr': 0.00043159509541709534, 'kernel_size': 2, 'num_channels_1': 104, 'num_channels_2': 69}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "[I 2025-03-05 16:07:19,528] Trial 29 finished with value: 0.007803649574634619 and parameters: {'dropout': 0.3798625204151019, 'lr': 1.448988139505015e-05, 'kernel_size': 5, 'num_channels_1': 55, 'num_channels_2': 63}. Best is trial 4 with value: 0.0032826566835865378.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:07:19,565] A new study created in memory with name: no-name-45054ea1-7bfc-48cd-8e40-6cffeb54800a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for T-shirt Black Solid: {'dropout': 0.10496665587160137, 'lr': 0.0005887969769196279, 'kernel_size': 5, 'num_channels_1': 60, 'num_channels_2': 51}\n",
      "\n",
      "=== Processing T-shirt White Solid ===\n",
      "ðŸ”„ Merging Google Trends data for T-shirt White Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:07:22,908] Trial 0 finished with value: 0.002451904921326786 and parameters: {'dropout': 0.39410760096327013, 'lr': 0.0006830865595600765, 'kernel_size': 2, 'num_channels_1': 105, 'num_channels_2': 33}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:27,328] Trial 1 finished with value: 0.00609288620762527 and parameters: {'dropout': 0.15596516065180877, 'lr': 1.6725611098627406e-05, 'kernel_size': 2, 'num_channels_1': 64, 'num_channels_2': 119}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:30,502] Trial 2 finished with value: 0.006744395918212831 and parameters: {'dropout': 0.10523483123187467, 'lr': 2.176859795964763e-05, 'kernel_size': 3, 'num_channels_1': 48, 'num_channels_2': 54}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:33,838] Trial 3 finished with value: 0.005294420081190765 and parameters: {'dropout': 0.3387685385903553, 'lr': 5.647906382365787e-05, 'kernel_size': 3, 'num_channels_1': 66, 'num_channels_2': 62}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:37,864] Trial 4 finished with value: 0.0063296595821157094 and parameters: {'dropout': 0.44457398008317617, 'lr': 4.5634420826446055e-05, 'kernel_size': 2, 'num_channels_1': 34, 'num_channels_2': 121}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:42,468] Trial 5 finished with value: 0.004891713242977857 and parameters: {'dropout': 0.161936010065102, 'lr': 5.360154271997316e-05, 'kernel_size': 2, 'num_channels_1': 61, 'num_channels_2': 76}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:47,527] Trial 6 finished with value: 0.002471304815844633 and parameters: {'dropout': 0.29116418268442046, 'lr': 0.00011295193651504995, 'kernel_size': 5, 'num_channels_1': 79, 'num_channels_2': 110}. Best is trial 0 with value: 0.002451904921326786.\n",
      "[I 2025-03-05 16:07:51,959] Trial 7 finished with value: 0.001874176535056904 and parameters: {'dropout': 0.280211873743468, 'lr': 0.00028122753337125265, 'kernel_size': 3, 'num_channels_1': 128, 'num_channels_2': 85}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:07:55,112] Trial 8 finished with value: 0.0019354534335434437 and parameters: {'dropout': 0.487803542256362, 'lr': 0.0005720667390717156, 'kernel_size': 3, 'num_channels_1': 98, 'num_channels_2': 42}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:00,532] Trial 9 finished with value: 0.005781018361449242 and parameters: {'dropout': 0.22584830855220234, 'lr': 1.4483788342051207e-05, 'kernel_size': 5, 'num_channels_1': 117, 'num_channels_2': 76}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:06,371] Trial 10 finished with value: 0.002339741273317486 and parameters: {'dropout': 0.2816474721219681, 'lr': 0.0002244744488907807, 'kernel_size': 4, 'num_channels_1': 123, 'num_channels_2': 97}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:09,827] Trial 11 finished with value: 0.00196494321571663 and parameters: {'dropout': 0.4640834646743597, 'lr': 0.0007384707536414279, 'kernel_size': 4, 'num_channels_1': 100, 'num_channels_2': 33}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:14,298] Trial 12 finished with value: 0.002072683034930378 and parameters: {'dropout': 0.35941988320969015, 'lr': 0.00027527360746385925, 'kernel_size': 3, 'num_channels_1': 96, 'num_channels_2': 95}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:18,782] Trial 13 finished with value: 0.0020562751102261244 and parameters: {'dropout': 0.23344214153313325, 'lr': 0.0003418350105781216, 'kernel_size': 4, 'num_channels_1': 128, 'num_channels_2': 58}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:22,966] Trial 14 finished with value: 0.0046854204498231414 and parameters: {'dropout': 0.3929195472278906, 'lr': 0.0009929287469416961, 'kernel_size': 3, 'num_channels_1': 86, 'num_channels_2': 90}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:26,409] Trial 15 finished with value: 0.0024829257861711087 and parameters: {'dropout': 0.4859973813298107, 'lr': 0.0004044798677826743, 'kernel_size': 3, 'num_channels_1': 111, 'num_channels_2': 48}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:30,218] Trial 16 finished with value: 0.002366932574659586 and parameters: {'dropout': 0.42158601349681124, 'lr': 0.00013572497122505518, 'kernel_size': 4, 'num_channels_1': 89, 'num_channels_2': 68}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:33,974] Trial 17 finished with value: 0.003318032482638955 and parameters: {'dropout': 0.25179633817314667, 'lr': 0.0001858344571291026, 'kernel_size': 3, 'num_channels_1': 113, 'num_channels_2': 45}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:38,648] Trial 18 finished with value: 0.0024630095111206176 and parameters: {'dropout': 0.33783826812804774, 'lr': 0.0005614982830955072, 'kernel_size': 4, 'num_channels_1': 77, 'num_channels_2': 85}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:43,567] Trial 19 finished with value: 0.002623222558759153 and parameters: {'dropout': 0.19274736030669276, 'lr': 0.00044836192290071547, 'kernel_size': 2, 'num_channels_1': 95, 'num_channels_2': 107}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:47,847] Trial 20 finished with value: 0.0021664735235390254 and parameters: {'dropout': 0.49899004602366276, 'lr': 0.00017511999829676123, 'kernel_size': 3, 'num_channels_1': 120, 'num_channels_2': 67}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:51,332] Trial 21 finished with value: 0.0026985430624336003 and parameters: {'dropout': 0.4775861641780667, 'lr': 0.0007201693481364954, 'kernel_size': 4, 'num_channels_1': 102, 'num_channels_2': 33}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:55,578] Trial 22 finished with value: 0.0024478281266056 and parameters: {'dropout': 0.44842439095459374, 'lr': 0.000986273902864467, 'kernel_size': 5, 'num_channels_1': 108, 'num_channels_2': 42}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:08:59,518] Trial 23 finished with value: 0.0027031481731683016 and parameters: {'dropout': 0.3929139077843564, 'lr': 0.0005023670023527777, 'kernel_size': 4, 'num_channels_1': 98, 'num_channels_2': 39}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:09:04,119] Trial 24 finished with value: 0.0027467345469631256 and parameters: {'dropout': 0.4524624320669999, 'lr': 0.0003284994506743445, 'kernel_size': 3, 'num_channels_1': 87, 'num_channels_2': 50}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:09:08,145] Trial 25 finished with value: 0.00337854630779475 and parameters: {'dropout': 0.3260672004228977, 'lr': 0.0007026607945857684, 'kernel_size': 4, 'num_channels_1': 73, 'num_channels_2': 81}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:09:12,190] Trial 26 finished with value: 0.0038308862596750258 and parameters: {'dropout': 0.3639841920875261, 'lr': 7.011097373470886e-05, 'kernel_size': 3, 'num_channels_1': 128, 'num_channels_2': 70}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:09:15,620] Trial 27 finished with value: 0.0024859321041731165 and parameters: {'dropout': 0.2673677813242128, 'lr': 0.00024427159192106176, 'kernel_size': 4, 'num_channels_1': 92, 'num_channels_2': 37}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:09:19,736] Trial 28 finished with value: 0.0022813032643171026 and parameters: {'dropout': 0.4222062825015581, 'lr': 0.0004906780864501894, 'kernel_size': 5, 'num_channels_1': 103, 'num_channels_2': 57}. Best is trial 7 with value: 0.001874176535056904.\n",
      "[I 2025-03-05 16:09:22,443] Trial 29 finished with value: 0.0021636426099576056 and parameters: {'dropout': 0.3115624975332192, 'lr': 0.000681985479945714, 'kernel_size': 2, 'num_channels_1': 107, 'num_channels_2': 32}. Best is trial 7 with value: 0.001874176535056904.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:09:22,479] A new study created in memory with name: no-name-7d2f8c35-880b-4f11-9c77-fcf10c1a77ab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for T-shirt White Solid: {'dropout': 0.280211873743468, 'lr': 0.00028122753337125265, 'kernel_size': 3, 'num_channels_1': 128, 'num_channels_2': 85}\n",
      "\n",
      "=== Processing Top Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Top Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:09:25,807] Trial 0 finished with value: 0.0019993568916106597 and parameters: {'dropout': 0.15523857813528924, 'lr': 6.0610348388241105e-05, 'kernel_size': 2, 'num_channels_1': 102, 'num_channels_2': 61}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:30,667] Trial 1 finished with value: 0.002866897126659751 and parameters: {'dropout': 0.4260348895906422, 'lr': 2.14776253590898e-05, 'kernel_size': 4, 'num_channels_1': 68, 'num_channels_2': 126}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:33,713] Trial 2 finished with value: 0.0029274300439283253 and parameters: {'dropout': 0.38631034473070214, 'lr': 3.5293810401399296e-05, 'kernel_size': 3, 'num_channels_1': 62, 'num_channels_2': 61}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:37,643] Trial 3 finished with value: 0.0034405728336423637 and parameters: {'dropout': 0.3401973862833941, 'lr': 1.820127288795487e-05, 'kernel_size': 3, 'num_channels_1': 103, 'num_channels_2': 75}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:41,998] Trial 4 finished with value: 0.002951060689520091 and parameters: {'dropout': 0.11936513654457924, 'lr': 1.0301692930044056e-05, 'kernel_size': 4, 'num_channels_1': 53, 'num_channels_2': 110}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:45,273] Trial 5 finished with value: 0.003601195407100022 and parameters: {'dropout': 0.40439540893094716, 'lr': 0.00010571057455158575, 'kernel_size': 3, 'num_channels_1': 32, 'num_channels_2': 84}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:49,318] Trial 6 finished with value: 0.0025657243328168987 and parameters: {'dropout': 0.4662957788095331, 'lr': 7.795500169230966e-05, 'kernel_size': 3, 'num_channels_1': 55, 'num_channels_2': 107}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:53,182] Trial 7 finished with value: 0.002142827771604061 and parameters: {'dropout': 0.43140811723724704, 'lr': 5.732383105092045e-05, 'kernel_size': 2, 'num_channels_1': 109, 'num_channels_2': 81}. Best is trial 0 with value: 0.0019993568916106597.\n",
      "[I 2025-03-05 16:09:57,799] Trial 8 finished with value: 0.0015219782922940794 and parameters: {'dropout': 0.31971026391812984, 'lr': 0.0007719653913583956, 'kernel_size': 2, 'num_channels_1': 111, 'num_channels_2': 96}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:02,042] Trial 9 finished with value: 0.001755308371502906 and parameters: {'dropout': 0.1944213133370453, 'lr': 0.00019379217463646656, 'kernel_size': 3, 'num_channels_1': 103, 'num_channels_2': 62}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:06,662] Trial 10 finished with value: 0.0025574633735232057 and parameters: {'dropout': 0.2606922275812026, 'lr': 0.0008257992743572236, 'kernel_size': 5, 'num_channels_1': 128, 'num_channels_2': 34}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:09,676] Trial 11 finished with value: 0.002283787971828133 and parameters: {'dropout': 0.2317063335583318, 'lr': 0.0006141543106132802, 'kernel_size': 2, 'num_channels_1': 88, 'num_channels_2': 39}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:14,604] Trial 12 finished with value: 0.0016196453259908594 and parameters: {'dropout': 0.21285559605612148, 'lr': 0.00026903477630144877, 'kernel_size': 2, 'num_channels_1': 125, 'num_channels_2': 97}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:20,009] Trial 13 finished with value: 0.0021633087773807345 and parameters: {'dropout': 0.31027281262128437, 'lr': 0.00034075340087584045, 'kernel_size': 2, 'num_channels_1': 128, 'num_channels_2': 99}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:24,522] Trial 14 finished with value: 0.001595246442593634 and parameters: {'dropout': 0.25455107323102943, 'lr': 0.00032692887725556765, 'kernel_size': 2, 'num_channels_1': 117, 'num_channels_2': 95}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:30,693] Trial 15 finished with value: 0.0026573588140308855 and parameters: {'dropout': 0.2912718084818786, 'lr': 0.00044361086495754964, 'kernel_size': 5, 'num_channels_1': 86, 'num_channels_2': 125}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:35,319] Trial 16 finished with value: 0.0016592180356383324 and parameters: {'dropout': 0.34622725151228095, 'lr': 0.0009596158078622297, 'kernel_size': 2, 'num_channels_1': 115, 'num_channels_2': 92}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:41,494] Trial 17 finished with value: 0.0020774749369593336 and parameters: {'dropout': 0.26775186211443164, 'lr': 0.00017020921505688338, 'kernel_size': 4, 'num_channels_1': 92, 'num_channels_2': 107}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:46,632] Trial 18 finished with value: 0.0017681724537396803 and parameters: {'dropout': 0.4901252010623364, 'lr': 0.00047028284291790077, 'kernel_size': 2, 'num_channels_1': 115, 'num_channels_2': 117}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:50,383] Trial 19 finished with value: 0.0023308940581046045 and parameters: {'dropout': 0.3371064475054242, 'lr': 0.00015134836378632646, 'kernel_size': 3, 'num_channels_1': 75, 'num_channels_2': 71}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:54,934] Trial 20 finished with value: 0.0017036881181411445 and parameters: {'dropout': 0.1689832302315869, 'lr': 0.0002769976671669275, 'kernel_size': 2, 'num_channels_1': 117, 'num_channels_2': 93}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:10:59,635] Trial 21 finished with value: 0.0016409623203799129 and parameters: {'dropout': 0.21461028208015348, 'lr': 0.0003269901370909414, 'kernel_size': 2, 'num_channels_1': 123, 'num_channels_2': 96}. Best is trial 8 with value: 0.0015219782922940794.\n",
      "[I 2025-03-05 16:11:04,008] Trial 22 finished with value: 0.001509397802874446 and parameters: {'dropout': 0.2468132197441826, 'lr': 0.0005973814223593947, 'kernel_size': 2, 'num_channels_1': 110, 'num_channels_2': 87}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:08,170] Trial 23 finished with value: 0.002031158877070993 and parameters: {'dropout': 0.25121895808906225, 'lr': 0.0006310186143561065, 'kernel_size': 2, 'num_channels_1': 97, 'num_channels_2': 85}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:12,249] Trial 24 finished with value: 0.0019516435451805592 and parameters: {'dropout': 0.29227278150246117, 'lr': 0.000999475568130626, 'kernel_size': 3, 'num_channels_1': 109, 'num_channels_2': 70}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:16,352] Trial 25 finished with value: 0.001850156590808183 and parameters: {'dropout': 0.36364052646813244, 'lr': 0.0006379817085076894, 'kernel_size': 2, 'num_channels_1': 81, 'num_channels_2': 90}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:21,451] Trial 26 finished with value: 0.0020327691687271 and parameters: {'dropout': 0.30936096178291694, 'lr': 0.00044160232671778675, 'kernel_size': 3, 'num_channels_1': 113, 'num_channels_2': 103}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:26,579] Trial 27 finished with value: 0.0016363979026209562 and parameters: {'dropout': 0.2427337143217003, 'lr': 0.00021794859433670678, 'kernel_size': 2, 'num_channels_1': 119, 'num_channels_2': 115}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:31,126] Trial 28 finished with value: 0.0020521277794614433 and parameters: {'dropout': 0.1718977524115293, 'lr': 0.0006757270358518909, 'kernel_size': 4, 'num_channels_1': 98, 'num_channels_2': 76}. Best is trial 22 with value: 0.001509397802874446.\n",
      "[I 2025-03-05 16:11:35,442] Trial 29 finished with value: 0.00197383948834613 and parameters: {'dropout': 0.12174300914081615, 'lr': 0.00013423703672180056, 'kernel_size': 2, 'num_channels_1': 108, 'num_channels_2': 88}. Best is trial 22 with value: 0.001509397802874446.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:11:35,481] A new study created in memory with name: no-name-530014cc-3f4f-4ef3-bad5-13bb4938ca5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Top Black Solid: {'dropout': 0.2468132197441826, 'lr': 0.0005973814223593947, 'kernel_size': 2, 'num_channels_1': 110, 'num_channels_2': 87}\n",
      "\n",
      "=== Processing Trousers Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Trousers Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:11:38,859] Trial 0 finished with value: 0.0016238623007666319 and parameters: {'dropout': 0.4356264244441783, 'lr': 1.7852627892922654e-05, 'kernel_size': 2, 'num_channels_1': 126, 'num_channels_2': 42}. Best is trial 0 with value: 0.0016238623007666319.\n",
      "[I 2025-03-05 16:11:42,785] Trial 1 finished with value: 0.0012555268243886531 and parameters: {'dropout': 0.3152339291665508, 'lr': 0.00027956767316223693, 'kernel_size': 3, 'num_channels_1': 80, 'num_channels_2': 78}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:11:47,486] Trial 2 finished with value: 0.0022070227714721113 and parameters: {'dropout': 0.3495200231221467, 'lr': 1.1122176346744802e-05, 'kernel_size': 3, 'num_channels_1': 81, 'num_channels_2': 109}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:11:51,662] Trial 3 finished with value: 0.0015955834067426621 and parameters: {'dropout': 0.1851716595181009, 'lr': 5.951797593605201e-05, 'kernel_size': 4, 'num_channels_1': 98, 'num_channels_2': 64}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:11:56,155] Trial 4 finished with value: 0.0027546844561584295 and parameters: {'dropout': 0.110207802734484, 'lr': 0.00039016185113553535, 'kernel_size': 2, 'num_channels_1': 85, 'num_channels_2': 104}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:00,728] Trial 5 finished with value: 0.0015603626379743218 and parameters: {'dropout': 0.492723922247993, 'lr': 0.0008785686912345811, 'kernel_size': 5, 'num_channels_1': 111, 'num_channels_2': 57}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:05,272] Trial 6 finished with value: 0.0017276300524827093 and parameters: {'dropout': 0.1986464709659327, 'lr': 9.098766028736564e-05, 'kernel_size': 2, 'num_channels_1': 98, 'num_channels_2': 107}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:11,036] Trial 7 finished with value: 0.00139602244598791 and parameters: {'dropout': 0.37423289447286257, 'lr': 2.9124577397115273e-05, 'kernel_size': 4, 'num_channels_1': 127, 'num_channels_2': 115}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:15,262] Trial 8 finished with value: 0.0014426977140828967 and parameters: {'dropout': 0.39531707110340586, 'lr': 0.00016860660618180806, 'kernel_size': 2, 'num_channels_1': 67, 'num_channels_2': 120}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:17,979] Trial 9 finished with value: 0.0033505450934171676 and parameters: {'dropout': 0.2449944778079429, 'lr': 3.2649340246717484e-05, 'kernel_size': 2, 'num_channels_1': 86, 'num_channels_2': 43}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:21,276] Trial 10 finished with value: 0.0012959201936610042 and parameters: {'dropout': 0.28820007906185624, 'lr': 0.00023421141166129496, 'kernel_size': 3, 'num_channels_1': 37, 'num_channels_2': 83}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:24,659] Trial 11 finished with value: 0.0017071846174076199 and parameters: {'dropout': 0.29089294148813455, 'lr': 0.000251821206932813, 'kernel_size': 3, 'num_channels_1': 37, 'num_channels_2': 87}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:27,849] Trial 12 finished with value: 0.0019484317279420793 and parameters: {'dropout': 0.29636816426127244, 'lr': 0.0006367723341196054, 'kernel_size': 3, 'num_channels_1': 33, 'num_channels_2': 85}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:31,146] Trial 13 finished with value: 0.0012916618143208325 and parameters: {'dropout': 0.32707436117906336, 'lr': 0.00017163809082886073, 'kernel_size': 4, 'num_channels_1': 57, 'num_channels_2': 71}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:34,698] Trial 14 finished with value: 0.0015283583663403988 and parameters: {'dropout': 0.35417804722967483, 'lr': 0.00013085988520690668, 'kernel_size': 5, 'num_channels_1': 59, 'num_channels_2': 69}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:38,334] Trial 15 finished with value: 0.0013953965943073853 and parameters: {'dropout': 0.23031316881111896, 'lr': 0.00038108803730686194, 'kernel_size': 4, 'num_channels_1': 57, 'num_channels_2': 72}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:42,330] Trial 16 finished with value: 0.0015221216366626323 and parameters: {'dropout': 0.4166426617113991, 'lr': 7.252009892938089e-05, 'kernel_size': 4, 'num_channels_1': 69, 'num_channels_2': 94}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:45,349] Trial 17 finished with value: 0.00197907870169729 and parameters: {'dropout': 0.33584462218659566, 'lr': 0.00043049008509553447, 'kernel_size': 5, 'num_channels_1': 50, 'num_channels_2': 51}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:49,145] Trial 18 finished with value: 0.0015406803111545741 and parameters: {'dropout': 0.4872459847757453, 'lr': 0.00017671047942547424, 'kernel_size': 4, 'num_channels_1': 71, 'num_channels_2': 75}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:52,834] Trial 19 finished with value: 0.0017715444322675467 and parameters: {'dropout': 0.1356916730453954, 'lr': 5.316994235955724e-05, 'kernel_size': 3, 'num_channels_1': 45, 'num_channels_2': 98}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:55,963] Trial 20 finished with value: 0.0013807374867610633 and parameters: {'dropout': 0.2606199226130574, 'lr': 0.00010785558428372393, 'kernel_size': 4, 'num_channels_1': 94, 'num_channels_2': 35}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:12:59,268] Trial 21 finished with value: 0.001357292803004384 and parameters: {'dropout': 0.33081713437210825, 'lr': 0.00024417348091124874, 'kernel_size': 3, 'num_channels_1': 43, 'num_channels_2': 81}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:13:02,232] Trial 22 finished with value: 0.0012936422426719218 and parameters: {'dropout': 0.28168899174857065, 'lr': 0.0002493554605297524, 'kernel_size': 3, 'num_channels_1': 55, 'num_channels_2': 60}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:13:05,224] Trial 23 finished with value: 0.0014573700842447578 and parameters: {'dropout': 0.27173008480710636, 'lr': 0.0005662753049848334, 'kernel_size': 3, 'num_channels_1': 58, 'num_channels_2': 59}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:13:08,486] Trial 24 finished with value: 0.001330210966989398 and parameters: {'dropout': 0.2113035538063019, 'lr': 0.0002772068594354409, 'kernel_size': 3, 'num_channels_1': 73, 'num_channels_2': 65}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:13:11,392] Trial 25 finished with value: 0.001477527676615864 and parameters: {'dropout': 0.3220881656044036, 'lr': 0.00014355722868268585, 'kernel_size': 4, 'num_channels_1': 51, 'num_channels_2': 52}. Best is trial 1 with value: 0.0012555268243886531.\n",
      "[I 2025-03-05 16:13:14,824] Trial 26 finished with value: 0.001194416463840753 and parameters: {'dropout': 0.1719881084700191, 'lr': 0.00030653747467052246, 'kernel_size': 3, 'num_channels_1': 64, 'num_channels_2': 76}. Best is trial 26 with value: 0.001194416463840753.\n",
      "[I 2025-03-05 16:13:18,808] Trial 27 finished with value: 0.0014349083183333277 and parameters: {'dropout': 0.15693718037007487, 'lr': 0.0009371370822066154, 'kernel_size': 4, 'num_channels_1': 64, 'num_channels_2': 92}. Best is trial 26 with value: 0.001194416463840753.\n",
      "[I 2025-03-05 16:13:22,971] Trial 28 finished with value: 0.0014489799359580502 and parameters: {'dropout': 0.16649645431939714, 'lr': 0.0005728177286914283, 'kernel_size': 5, 'num_channels_1': 77, 'num_channels_2': 76}. Best is trial 26 with value: 0.001194416463840753.\n",
      "[I 2025-03-05 16:13:26,803] Trial 29 finished with value: 0.0018740257597528397 and parameters: {'dropout': 0.4631488101719422, 'lr': 0.00033143412851683885, 'kernel_size': 2, 'num_channels_1': 113, 'num_channels_2': 77}. Best is trial 26 with value: 0.001194416463840753.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:13:26,840] A new study created in memory with name: no-name-05b128f6-c0ce-44eb-a61c-261f9fc733ec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Trousers Black Solid: {'dropout': 0.1719881084700191, 'lr': 0.00030653747467052246, 'kernel_size': 3, 'num_channels_1': 64, 'num_channels_2': 76}\n",
      "\n",
      "=== Processing Trousers Blue Denim ===\n",
      "ðŸ”„ Merging Google Trends data for Trousers Blue Denim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:13:31,850] Trial 0 finished with value: 0.003875650046393275 and parameters: {'dropout': 0.19264658648690602, 'lr': 0.0002014243438665461, 'kernel_size': 5, 'num_channels_1': 66, 'num_channels_2': 121}. Best is trial 0 with value: 0.003875650046393275.\n",
      "[I 2025-03-05 16:13:37,740] Trial 1 finished with value: 0.004417493799701333 and parameters: {'dropout': 0.4433308238044088, 'lr': 1.631267503134161e-05, 'kernel_size': 5, 'num_channels_1': 118, 'num_channels_2': 119}. Best is trial 0 with value: 0.003875650046393275.\n",
      "[I 2025-03-05 16:13:41,444] Trial 2 finished with value: 0.0036088132299482824 and parameters: {'dropout': 0.2894757402557753, 'lr': 0.0002042442261173503, 'kernel_size': 2, 'num_channels_1': 73, 'num_channels_2': 74}. Best is trial 2 with value: 0.0036088132299482824.\n",
      "[I 2025-03-05 16:13:45,750] Trial 3 finished with value: 0.004081763979047537 and parameters: {'dropout': 0.26325757335067757, 'lr': 0.0003675039650574493, 'kernel_size': 5, 'num_channels_1': 55, 'num_channels_2': 65}. Best is trial 2 with value: 0.0036088132299482824.\n",
      "[I 2025-03-05 16:13:51,870] Trial 4 finished with value: 0.003241158148739487 and parameters: {'dropout': 0.3711150648126008, 'lr': 0.0009372270704658072, 'kernel_size': 3, 'num_channels_1': 101, 'num_channels_2': 123}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:13:59,381] Trial 5 finished with value: 0.0038923681247979404 and parameters: {'dropout': 0.17135581897560317, 'lr': 1.0536454057686136e-05, 'kernel_size': 5, 'num_channels_1': 126, 'num_channels_2': 126}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:02,976] Trial 6 finished with value: 0.003977269935421646 and parameters: {'dropout': 0.372324476189409, 'lr': 6.0224680411477145e-05, 'kernel_size': 2, 'num_channels_1': 33, 'num_channels_2': 53}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:08,158] Trial 7 finished with value: 0.0048063244903460145 and parameters: {'dropout': 0.22428385659650482, 'lr': 1.6317837112868105e-05, 'kernel_size': 3, 'num_channels_1': 120, 'num_channels_2': 68}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:13,704] Trial 8 finished with value: 0.005567590892314911 and parameters: {'dropout': 0.3300256975808052, 'lr': 2.005200112970277e-05, 'kernel_size': 2, 'num_channels_1': 117, 'num_channels_2': 90}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:19,774] Trial 9 finished with value: 0.004063522396609187 and parameters: {'dropout': 0.29767444099208307, 'lr': 0.000281423224958786, 'kernel_size': 5, 'num_channels_1': 110, 'num_channels_2': 72}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:23,242] Trial 10 finished with value: 0.003759157203603536 and parameters: {'dropout': 0.49741215671901223, 'lr': 0.0009482758873975744, 'kernel_size': 3, 'num_channels_1': 94, 'num_channels_2': 32}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:29,472] Trial 11 finished with value: 0.004038938891608268 and parameters: {'dropout': 0.112339838121774, 'lr': 0.000977056865555876, 'kernel_size': 3, 'num_channels_1': 85, 'num_channels_2': 98}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:34,692] Trial 12 finished with value: 0.003604829264804721 and parameters: {'dropout': 0.38861954600206994, 'lr': 8.93192059188593e-05, 'kernel_size': 2, 'num_channels_1': 100, 'num_channels_2': 101}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:39,937] Trial 13 finished with value: 0.005311726592481136 and parameters: {'dropout': 0.40217438162392716, 'lr': 4.5823464553488435e-05, 'kernel_size': 4, 'num_channels_1': 100, 'num_channels_2': 104}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:45,899] Trial 14 finished with value: 0.005258967122063041 and parameters: {'dropout': 0.36425551458848693, 'lr': 8.945133400724105e-05, 'kernel_size': 4, 'num_channels_1': 103, 'num_channels_2': 109}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:50,575] Trial 15 finished with value: 0.0036270151380449534 and parameters: {'dropout': 0.4330405023692097, 'lr': 0.00012636495342392858, 'kernel_size': 2, 'num_channels_1': 87, 'num_channels_2': 90}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:14:56,727] Trial 16 finished with value: 0.0037560402299277483 and parameters: {'dropout': 0.49882753142513775, 'lr': 0.0005562318082864208, 'kernel_size': 3, 'num_channels_1': 105, 'num_channels_2': 128}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:15:03,121] Trial 17 finished with value: 0.00385946836322546 and parameters: {'dropout': 0.34765177675304704, 'lr': 3.762172631306051e-05, 'kernel_size': 4, 'num_channels_1': 92, 'num_channels_2': 110}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:15:06,919] Trial 18 finished with value: 0.004386710561811924 and parameters: {'dropout': 0.4207017302731541, 'lr': 0.00010467837694437441, 'kernel_size': 2, 'num_channels_1': 70, 'num_channels_2': 91}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:15:11,527] Trial 19 finished with value: 0.004606078413780778 and parameters: {'dropout': 0.3930794909511934, 'lr': 0.0004881347978169634, 'kernel_size': 3, 'num_channels_1': 79, 'num_channels_2': 115}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:15:15,236] Trial 20 finished with value: 0.005086976382881403 and parameters: {'dropout': 0.3242584542709199, 'lr': 0.00016292426885305698, 'kernel_size': 2, 'num_channels_1': 60, 'num_channels_2': 99}. Best is trial 4 with value: 0.003241158148739487.\n",
      "[I 2025-03-05 16:15:18,992] Trial 21 finished with value: 0.002962297620251775 and parameters: {'dropout': 0.285029565834012, 'lr': 0.00022808708617784595, 'kernel_size': 2, 'num_channels_1': 74, 'num_channels_2': 79}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:22,969] Trial 22 finished with value: 0.004509484115988016 and parameters: {'dropout': 0.25692208882516243, 'lr': 6.471084069995378e-05, 'kernel_size': 2, 'num_channels_1': 48, 'num_channels_2': 84}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:26,274] Trial 23 finished with value: 0.0031598732806742192 and parameters: {'dropout': 0.46077733905741886, 'lr': 0.0005353132396268081, 'kernel_size': 3, 'num_channels_1': 81, 'num_channels_2': 57}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:29,472] Trial 24 finished with value: 0.0034092144574970006 and parameters: {'dropout': 0.4579905886911999, 'lr': 0.000582347552656035, 'kernel_size': 3, 'num_channels_1': 77, 'num_channels_2': 53}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:33,165] Trial 25 finished with value: 0.003334788605570793 and parameters: {'dropout': 0.46683937027678063, 'lr': 0.00033297421489352695, 'kernel_size': 4, 'num_channels_1': 84, 'num_channels_2': 57}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:36,307] Trial 26 finished with value: 0.0034732440253719687 and parameters: {'dropout': 0.2596233111349967, 'lr': 0.0007025462186388594, 'kernel_size': 3, 'num_channels_1': 93, 'num_channels_2': 45}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:39,970] Trial 27 finished with value: 0.004037318332120776 and parameters: {'dropout': 0.328457378858363, 'lr': 0.0002706203596895571, 'kernel_size': 3, 'num_channels_1': 64, 'num_channels_2': 81}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:42,677] Trial 28 finished with value: 0.003585697431117296 and parameters: {'dropout': 0.2232203048332584, 'lr': 0.0004063763708647023, 'kernel_size': 4, 'num_channels_1': 46, 'num_channels_2': 41}. Best is trial 21 with value: 0.002962297620251775.\n",
      "[I 2025-03-05 16:15:45,898] Trial 29 finished with value: 0.0033839836018159986 and parameters: {'dropout': 0.15119095173246255, 'lr': 0.0008728760022035161, 'kernel_size': 3, 'num_channels_1': 71, 'num_channels_2': 61}. Best is trial 21 with value: 0.002962297620251775.\n",
      "/var/folders/33/sqxmhw8j72g_q70vx5jqs_jr0000gn/T/ipykernel_31890/3984781898.py:86: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  product_data.fillna(method='ffill', inplace=True)\n",
      "[I 2025-03-05 16:15:45,936] A new study created in memory with name: no-name-3207419a-4056-4c29-a72d-654b10903914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Trousers Blue Denim: {'dropout': 0.285029565834012, 'lr': 0.00022808708617784595, 'kernel_size': 2, 'num_channels_1': 74, 'num_channels_2': 79}\n",
      "\n",
      "=== Processing Vest top Black Solid ===\n",
      "ðŸ”„ Merging Google Trends data for Vest top Black Solid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-05 16:15:50,424] Trial 0 finished with value: 0.005543003417551517 and parameters: {'dropout': 0.23490553642845918, 'lr': 6.0216905788401526e-05, 'kernel_size': 2, 'num_channels_1': 100, 'num_channels_2': 112}. Best is trial 0 with value: 0.005543003417551517.\n",
      "[I 2025-03-05 16:15:54,118] Trial 1 finished with value: 0.012048532068729401 and parameters: {'dropout': 0.43457733044232805, 'lr': 2.9706778853279197e-05, 'kernel_size': 2, 'num_channels_1': 49, 'num_channels_2': 97}. Best is trial 0 with value: 0.005543003417551517.\n",
      "[I 2025-03-05 16:16:00,185] Trial 2 finished with value: 0.003604222519788891 and parameters: {'dropout': 0.48039440145201895, 'lr': 0.00031298508968597176, 'kernel_size': 5, 'num_channels_1': 107, 'num_channels_2': 128}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:03,493] Trial 3 finished with value: 0.010892627015709876 and parameters: {'dropout': 0.3638158084324513, 'lr': 1.9186972290596043e-05, 'kernel_size': 4, 'num_channels_1': 66, 'num_channels_2': 51}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:07,568] Trial 4 finished with value: 0.003795550623908639 and parameters: {'dropout': 0.3759075972138517, 'lr': 0.0006059626217093074, 'kernel_size': 2, 'num_channels_1': 116, 'num_channels_2': 79}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:12,291] Trial 5 finished with value: 0.007167876604944468 and parameters: {'dropout': 0.11555650452699454, 'lr': 7.850650183883103e-05, 'kernel_size': 4, 'num_channels_1': 68, 'num_channels_2': 110}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:17,024] Trial 6 finished with value: 0.0037256992887705564 and parameters: {'dropout': 0.3775960066466465, 'lr': 0.0001412459949936228, 'kernel_size': 2, 'num_channels_1': 85, 'num_channels_2': 126}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:20,197] Trial 7 finished with value: 0.009273308515548705 and parameters: {'dropout': 0.4213266484731676, 'lr': 0.0008457778907609365, 'kernel_size': 3, 'num_channels_1': 104, 'num_channels_2': 38}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:23,609] Trial 8 finished with value: 0.004983285907655954 and parameters: {'dropout': 0.10339919340776996, 'lr': 0.00010767771466378046, 'kernel_size': 4, 'num_channels_1': 53, 'num_channels_2': 57}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:29,596] Trial 9 finished with value: 0.004117537196725607 and parameters: {'dropout': 0.285648841412354, 'lr': 0.00019048591776535948, 'kernel_size': 4, 'num_channels_1': 128, 'num_channels_2': 120}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:34,370] Trial 10 finished with value: 0.005187626369297505 and parameters: {'dropout': 0.49590737920634814, 'lr': 0.00033529485466407376, 'kernel_size': 5, 'num_channels_1': 86, 'num_channels_2': 84}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:40,675] Trial 11 finished with value: 0.004056815209332853 and parameters: {'dropout': 0.4845677217648477, 'lr': 0.00023543569243981606, 'kernel_size': 5, 'num_channels_1': 88, 'num_channels_2': 128}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:44,933] Trial 12 finished with value: 0.004478911822661758 and parameters: {'dropout': 0.33390296471108993, 'lr': 0.00039607786932840543, 'kernel_size': 3, 'num_channels_1': 34, 'num_channels_2': 128}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:49,649] Trial 13 finished with value: 0.004266794351860881 and parameters: {'dropout': 0.42262526533361444, 'lr': 0.00013652268767014208, 'kernel_size': 3, 'num_channels_1': 108, 'num_channels_2': 99}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:55,498] Trial 14 finished with value: 0.005997915612533688 and parameters: {'dropout': 0.2592637577707966, 'lr': 4.605200177728376e-05, 'kernel_size': 5, 'num_channels_1': 93, 'num_channels_2': 105}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:16:59,832] Trial 15 finished with value: 0.013292005844414234 and parameters: {'dropout': 0.45631065551366473, 'lr': 1.1959236951435739e-05, 'kernel_size': 3, 'num_channels_1': 78, 'num_channels_2': 86}. Best is trial 2 with value: 0.003604222519788891.\n",
      "[I 2025-03-05 16:17:05,070] Trial 16 finished with value: 0.0033286384772509337 and parameters: {'dropout': 0.20095300077236217, 'lr': 0.0003939519305054854, 'kernel_size': 2, 'num_channels_1': 121, 'num_channels_2': 114}. Best is trial 16 with value: 0.0033286384772509337.\n",
      "[I 2025-03-05 16:17:10,104] Trial 17 finished with value: 0.004658374143764377 and parameters: {'dropout': 0.2077755348375638, 'lr': 0.0004780020277054007, 'kernel_size': 5, 'num_channels_1': 123, 'num_channels_2': 68}. Best is trial 16 with value: 0.0033286384772509337.\n",
      "[I 2025-03-05 16:17:15,401] Trial 18 finished with value: 0.004718201828654856 and parameters: {'dropout': 0.16872255739079797, 'lr': 0.0008180132712155602, 'kernel_size': 4, 'num_channels_1': 115, 'num_channels_2': 113}. Best is trial 16 with value: 0.0033286384772509337.\n",
      "[I 2025-03-05 16:17:19,962] Trial 19 finished with value: 0.0035567285027354955 and parameters: {'dropout': 0.16638779464929263, 'lr': 0.0002802572137425512, 'kernel_size': 3, 'num_channels_1': 114, 'num_channels_2': 97}. Best is trial 16 with value: 0.0033286384772509337.\n",
      "[I 2025-03-05 16:17:24,235] Trial 20 finished with value: 0.004480208549648523 and parameters: {'dropout': 0.16175021314533117, 'lr': 0.00021981225376225614, 'kernel_size': 2, 'num_channels_1': 119, 'num_channels_2': 93}. Best is trial 16 with value: 0.0033286384772509337.\n",
      "[I 2025-03-05 16:17:29,215] Trial 21 finished with value: 0.003507423587143421 and parameters: {'dropout': 0.1634155949773985, 'lr': 0.0002903911368417053, 'kernel_size': 3, 'num_channels_1': 110, 'num_channels_2': 118}. Best is trial 16 with value: 0.0033286384772509337.\n",
      "[I 2025-03-05 16:17:33,770] Trial 22 finished with value: 0.002918573055649176 and parameters: {'dropout': 0.1737368202829573, 'lr': 0.0005002213532302297, 'kernel_size': 3, 'num_channels_1': 97, 'num_channels_2': 104}. Best is trial 22 with value: 0.002918573055649176.\n",
      "[I 2025-03-05 16:17:38,439] Trial 23 finished with value: 0.01001303847442614 and parameters: {'dropout': 0.20485175770347452, 'lr': 0.0005615486683501096, 'kernel_size': 3, 'num_channels_1': 96, 'num_channels_2': 115}. Best is trial 22 with value: 0.002918573055649176.\n",
      "[I 2025-03-05 16:17:42,964] Trial 24 finished with value: 0.0029855067143216727 and parameters: {'dropout': 0.13696580852329185, 'lr': 0.0006582453911713453, 'kernel_size': 2, 'num_channels_1': 124, 'num_channels_2': 104}. Best is trial 22 with value: 0.002918573055649176.\n",
      "[I 2025-03-05 16:17:47,554] Trial 25 finished with value: 0.004398184805177152 and parameters: {'dropout': 0.13581631193883326, 'lr': 0.0009501475367697076, 'kernel_size': 2, 'num_channels_1': 127, 'num_channels_2': 105}. Best is trial 22 with value: 0.002918573055649176.\n",
      "[I 2025-03-05 16:17:51,786] Trial 26 finished with value: 0.002894781599752605 and parameters: {'dropout': 0.2071217321162468, 'lr': 0.0006427005870557553, 'kernel_size': 2, 'num_channels_1': 122, 'num_channels_2': 90}. Best is trial 26 with value: 0.002894781599752605.\n",
      "[I 2025-03-05 16:17:55,468] Trial 27 finished with value: 0.002899236953817308 and parameters: {'dropout': 0.25379828959214934, 'lr': 0.0006395759422357452, 'kernel_size': 2, 'num_channels_1': 101, 'num_channels_2': 74}. Best is trial 26 with value: 0.002894781599752605.\n",
      "[I 2025-03-05 16:17:59,224] Trial 28 finished with value: 0.003728633653372526 and parameters: {'dropout': 0.26769042420030026, 'lr': 0.00048465392753563324, 'kernel_size': 2, 'num_channels_1': 76, 'num_channels_2': 74}. Best is trial 26 with value: 0.002894781599752605.\n",
      "[I 2025-03-05 16:18:02,740] Trial 29 finished with value: 0.002975571365095675 and parameters: {'dropout': 0.23905199410837888, 'lr': 0.0007466801858231842, 'kernel_size': 2, 'num_channels_1': 101, 'num_channels_2': 67}. Best is trial 26 with value: 0.002894781599752605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Vest top Black Solid: {'dropout': 0.2071217321162468, 'lr': 0.0006427005870557553, 'kernel_size': 2, 'num_channels_1': 122, 'num_channels_2': 90}\n",
      "\n",
      "âœ… All Google Trends TCN processing with Optuna tuning completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/google_trends/1_day/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def objective_fn(trial, train_loader, val_loader, num_features):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 5)\n",
    "    num_channels = [trial.suggest_int(\"num_channels_1\", 32, 128), trial.suggest_int(\"num_channels_2\", 32, 128)]\n",
    "    \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=num_channels,\n",
    "        output_size=1,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(20):  # Reduced epochs for tuning\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Filter data for this product group\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    if product_data.empty:\n",
    "        print(f\"No data for {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "    product_data.set_index('date', inplace=True)\n",
    "    product_data = product_data.asfreq('D').fillna(0)\n",
    "\n",
    "    # 2. Merge Google Trends Data\n",
    "    print(f\"ðŸ”„ Merging Google Trends data for {product_group}...\")\n",
    "    for col in ['product_type_name', 'colour_group_name', 'graphical_appearance_name']:\n",
    "        if col in product_data.columns:\n",
    "            val = product_data[col].iloc[0]\n",
    "            if val in keywords_list:\n",
    "                trend_file = get_trends_file(val)\n",
    "                if trend_file:\n",
    "                    trend_data = load_and_process_trends(trend_file, product_data)\n",
    "                    product_data = add_trends_to_product_data(product_data, trend_data, val)\n",
    "    product_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # 3. Drop unwanted columns\n",
    "    drop_cols = ['product_group', 'product_type_name', 'colour_group_name', 'graphical_appearance_name']\n",
    "    product_data.drop(columns=[c for c in drop_cols if c in product_data.columns], inplace=True, errors='ignore')\n",
    "\n",
    "    if 'transaction_count' not in product_data.columns:\n",
    "        print(f\"No transaction_count in {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 4. Train/Validation Split (80/20)\n",
    "    total_len = len(product_data)\n",
    "    if total_len < 50:\n",
    "        print(f\"Not enough data for {product_group}, skipping.\")\n",
    "        continue\n",
    "    split_idx = int(0.8 * total_len)\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "\n",
    "    # 5. Scale features & target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = train_df.drop(columns=['transaction_count'])\n",
    "    y_train = train_df[['transaction_count']]\n",
    "\n",
    "    X_val = val_df.drop(columns=['transaction_count'])\n",
    "    y_val = val_df[['transaction_count']]\n",
    "\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "\n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "\n",
    "    # 6. Create TimeSeriesDataset\n",
    "    input_window, output_window = 14, 1\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective_fn(trial, train_loader, val_loader, X_train.shape[1] + 1), n_trials=30)\n",
    "    best_params = study.best_trial.params\n",
    "    print(f\"Best parameters for {product_group}: {best_params}\")\n",
    "\n",
    "    # Save best hyperparameters\n",
    "    pd.DataFrame([best_params]).to_csv(os.path.join(group_output_dir, \"best_hyperparameters.csv\"), index=False)\n",
    "\n",
    "print(\"\\nâœ… All Google Trends TCN processing with Optuna tuning completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Bra Black Solid ===\n",
      "\n",
      "=== Processing Dress Black Solid ===\n",
      "\n",
      "=== Processing Leggings/Tights Black Solid ===\n",
      "\n",
      "=== Processing Sweater Black Solid ===\n",
      "\n",
      "=== Processing T-shirt Black Solid ===\n",
      "\n",
      "=== Processing T-shirt White Solid ===\n",
      "\n",
      "=== Processing Top Black Solid ===\n",
      "\n",
      "=== Processing Trousers Black Solid ===\n",
      "\n",
      "=== Processing Trousers Blue Denim ===\n",
      "\n",
      "=== Processing Vest top Black Solid ===\n",
      "\n",
      "âœ… All Google Trends TCN processing with Optuna tuning, best model training, evaluation, and summary completed. Results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megan/Thesis/thesis_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/megan/Thesis/thesis_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import optuna\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/google_trends/1_day/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def objective_fn(trial, train_loader, val_loader, num_features):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 5)\n",
    "    num_channels = [trial.suggest_int(\"num_channels_1\", 32, 128), trial.suggest_int(\"num_channels_2\", 32, 128)]\n",
    "    \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=num_channels,\n",
    "        output_size=1,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(20):  # Reduced epochs for tuning\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna for all product groups\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "\n",
    "    # Load best hyperparameters\n",
    "    best_params_path = os.path.join(group_output_dir, \"best_hyperparameters.csv\")\n",
    "    if not os.path.exists(best_params_path):\n",
    "        print(f\"Skipping {product_group}, no best hyperparameters found.\")\n",
    "        continue\n",
    "\n",
    "    best_params = pd.read_csv(best_params_path).iloc[0].to_dict()\n",
    "\n",
    "    # Train model with best hyperparameters\n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=X_train.shape[1] + 1,\n",
    "        num_channels=[int(best_params['num_channels_1']), int(best_params['num_channels_2'])],\n",
    "        output_size=1,\n",
    "        kernel_size=int(best_params['kernel_size']),\n",
    "        dropout=best_params['dropout']\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Save trained model\n",
    "    torch.save(model.state_dict(), os.path.join(group_output_dir, \"best_model.pth\"))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Compute metrics\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(group_output_dir, \"test_metrics.csv\"), index=False)\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary metrics saved to: final_version/output/google_trends/1_day/tcn/final_metrics_summary.csv\n",
      "Average metrics saved to: final_version/output/google_trends/1_day/tcn/final_test_avg_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = \"final_version/output/google_trends/1_day/tcn\"\n",
    "\n",
    "metrics_summary = []\n",
    "\n",
    "# Gather metrics from each product group's folder\n",
    "for product_group in top_10_groups:\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    metrics_file = os.path.join(group_output_dir, \"test_metrics.csv\")\n",
    "    \n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file)\n",
    "        # Assume test_metrics.csv has columns: MAE, RMSE, MAPE, R2\n",
    "        # and just one row of metrics\n",
    "        row = df.iloc[0].to_dict()\n",
    "        row[\"Product Group\"] = product_group\n",
    "        metrics_summary.append(row)\n",
    "    else:\n",
    "        print(f\"Warning: No metrics file found for {product_group}\")\n",
    "\n",
    "# Convert to a DataFrame\n",
    "summary_df = pd.DataFrame(metrics_summary)\n",
    "\n",
    "# Reorder columns for clarity\n",
    "cols_order = [\"Product Group\", \"MAE\", \"RMSE\", \"MAPE\", \"R2\"]\n",
    "summary_df = summary_df[cols_order]\n",
    "\n",
    "# Save the summary of all product groups\n",
    "summary_file = os.path.join(output_dir, \"final_metrics_summary.csv\")\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "# Compute and save average metrics across all products\n",
    "avg_metrics = summary_df[[\"MAE\", \"RMSE\", \"MAPE\", \"R2\"]].mean()\n",
    "avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "avg_metrics_file = os.path.join(output_dir, \"final_test_avg_metrics.csv\")\n",
    "avg_metrics_df.to_csv(avg_metrics_file, index=False)\n",
    "\n",
    "print(f\"Summary metrics saved to: {summary_file}\")\n",
    "print(f\"Average metrics saved to: {avg_metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macroeconomic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_sentiment = pd.read_csv('data/external/consumer_sentiment.csv')\n",
    "consumer_sentiment['DATE'] = pd.to_datetime(consumer_sentiment['DATE'])\n",
    "consumer_sentiment.set_index('DATE', inplace=True)\n",
    "consumer_sentiment = consumer_sentiment.resample('D').ffill()\n",
    "\n",
    "cpi = pd.read_csv('data/external/cpi_data.csv')\n",
    "cpi['DATE'] = pd.to_datetime(cpi['DATE'])\n",
    "cpi.set_index('DATE', inplace=True)\n",
    "cpi = cpi.resample('D').ffill()\n",
    "\n",
    "gdp = pd.read_csv('data/external/gdp_data.csv')\n",
    "gdp['DATE'] = pd.to_datetime(gdp['DATE'])\n",
    "gdp.set_index('DATE', inplace=True)\n",
    "gdp = gdp.resample('D').ffill()\n",
    "\n",
    "unemployment = pd.read_csv('data/external/unemployment_data.csv')\n",
    "unemployment['DATE'] = pd.to_datetime(unemployment['DATE'])\n",
    "unemployment.set_index('DATE', inplace=True)\n",
    "unemployment = unemployment.resample('D').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'product_group', 'transaction_count', 'avg_price',\n",
       "       'sales_channel', 'unique_customers', 'unique_articles_sold',\n",
       "       'median_age', 'fashion_news_subscribers', 'first_purchase_days_ago',\n",
       "       'recent_purchase_days_ago', 'age_bin_10-19', 'age_bin_20-29',\n",
       "       'age_bin_30-39', 'age_bin_40-49', 'age_bin_50-59', 'age_bin_60+'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/top_10_product_groups.csv')\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Bra Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Bra Black Solid...\n",
      "Bra Black Solid | Epoch 10/50, Train Loss: 0.007396, Val Loss: 0.004328\n",
      "Bra Black Solid | Epoch 20/50, Train Loss: 0.006508, Val Loss: 0.005507\n",
      "Bra Black Solid | Epoch 30/50, Train Loss: 0.006304, Val Loss: 0.006876\n",
      "Bra Black Solid | Epoch 40/50, Train Loss: 0.006162, Val Loss: 0.007880\n",
      "Bra Black Solid | Epoch 50/50, Train Loss: 0.005928, Val Loss: 0.008154\n",
      "Bra Black Solid => MAE: 218.5691, RMSE: 249.5633, MAPE: 43.33%, R2: -0.5764\n",
      "\n",
      "=== Processing Dress Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Dress Black Solid...\n",
      "Dress Black Solid | Epoch 10/50, Train Loss: 0.016218, Val Loss: 0.017863\n",
      "Dress Black Solid | Epoch 20/50, Train Loss: 0.013352, Val Loss: 0.013471\n",
      "Dress Black Solid | Epoch 30/50, Train Loss: 0.012026, Val Loss: 0.013320\n",
      "Dress Black Solid | Epoch 40/50, Train Loss: 0.011832, Val Loss: 0.012571\n",
      "Dress Black Solid | Epoch 50/50, Train Loss: 0.010317, Val Loss: 0.011828\n",
      "Dress Black Solid => MAE: 331.9651, RMSE: 432.5531, MAPE: 33.53%, R2: 0.3857\n",
      "\n",
      "=== Processing Leggings/Tights Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Leggings/Tights Black Solid...\n",
      "Leggings/Tights Black Solid | Epoch 10/50, Train Loss: 0.008324, Val Loss: 0.002716\n",
      "Leggings/Tights Black Solid | Epoch 20/50, Train Loss: 0.007292, Val Loss: 0.002419\n",
      "Leggings/Tights Black Solid | Epoch 30/50, Train Loss: 0.005999, Val Loss: 0.002344\n",
      "Leggings/Tights Black Solid | Epoch 40/50, Train Loss: 0.005601, Val Loss: 0.002270\n",
      "Leggings/Tights Black Solid | Epoch 50/50, Train Loss: 0.005037, Val Loss: 0.002273\n",
      "Leggings/Tights Black Solid => MAE: 110.1060, RMSE: 133.0172, MAPE: 31.06%, R2: 0.3285\n",
      "\n",
      "=== Processing Sweater Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Sweater Black Solid...\n",
      "Sweater Black Solid | Epoch 10/50, Train Loss: 0.004538, Val Loss: 0.001916\n",
      "Sweater Black Solid | Epoch 20/50, Train Loss: 0.003857, Val Loss: 0.001224\n",
      "Sweater Black Solid | Epoch 30/50, Train Loss: 0.003274, Val Loss: 0.001064\n",
      "Sweater Black Solid | Epoch 40/50, Train Loss: 0.003121, Val Loss: 0.001054\n",
      "Sweater Black Solid | Epoch 50/50, Train Loss: 0.002997, Val Loss: 0.001049\n",
      "Sweater Black Solid => MAE: 162.9683, RMSE: 212.8501, MAPE: 83.98%, R2: 0.6082\n",
      "\n",
      "=== Processing T-shirt Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for T-shirt Black Solid...\n",
      "T-shirt Black Solid | Epoch 10/50, Train Loss: 0.012857, Val Loss: 0.004475\n",
      "T-shirt Black Solid | Epoch 20/50, Train Loss: 0.010774, Val Loss: 0.003249\n",
      "T-shirt Black Solid | Epoch 30/50, Train Loss: 0.008776, Val Loss: 0.003467\n",
      "T-shirt Black Solid | Epoch 40/50, Train Loss: 0.009772, Val Loss: 0.003315\n",
      "T-shirt Black Solid | Epoch 50/50, Train Loss: 0.009007, Val Loss: 0.003091\n",
      "T-shirt Black Solid => MAE: 99.0299, RMSE: 131.6931, MAPE: 18.03%, R2: 0.1647\n",
      "\n",
      "=== Processing T-shirt White Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for T-shirt White Solid...\n",
      "T-shirt White Solid | Epoch 10/50, Train Loss: 0.010824, Val Loss: 0.007087\n",
      "T-shirt White Solid | Epoch 20/50, Train Loss: 0.007830, Val Loss: 0.003240\n",
      "T-shirt White Solid | Epoch 30/50, Train Loss: 0.006705, Val Loss: 0.004000\n",
      "T-shirt White Solid | Epoch 40/50, Train Loss: 0.006309, Val Loss: 0.004190\n",
      "T-shirt White Solid | Epoch 50/50, Train Loss: 0.005697, Val Loss: 0.004459\n",
      "T-shirt White Solid => MAE: 136.9634, RMSE: 159.1878, MAPE: 29.39%, R2: 0.3759\n",
      "\n",
      "=== Processing Top Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Top Black Solid...\n",
      "Top Black Solid | Epoch 10/50, Train Loss: 0.005157, Val Loss: 0.003517\n",
      "Top Black Solid | Epoch 20/50, Train Loss: 0.004701, Val Loss: 0.003334\n",
      "Top Black Solid | Epoch 30/50, Train Loss: 0.004624, Val Loss: 0.003961\n",
      "Top Black Solid | Epoch 40/50, Train Loss: 0.004277, Val Loss: 0.004275\n",
      "Top Black Solid | Epoch 50/50, Train Loss: 0.004246, Val Loss: 0.004703\n",
      "Top Black Solid => MAE: 226.3938, RMSE: 267.9997, MAPE: 30.58%, R2: -0.6537\n",
      "\n",
      "=== Processing Trousers Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Trousers Black Solid...\n",
      "Trousers Black Solid | Epoch 10/50, Train Loss: 0.009408, Val Loss: 0.002085\n",
      "Trousers Black Solid | Epoch 20/50, Train Loss: 0.008800, Val Loss: 0.001923\n",
      "Trousers Black Solid | Epoch 30/50, Train Loss: 0.008967, Val Loss: 0.001709\n",
      "Trousers Black Solid | Epoch 40/50, Train Loss: 0.007789, Val Loss: 0.001313\n",
      "Trousers Black Solid | Epoch 50/50, Train Loss: 0.007663, Val Loss: 0.001157\n",
      "Trousers Black Solid => MAE: 192.1398, RMSE: 251.1456, MAPE: 18.54%, R2: 0.2153\n",
      "\n",
      "=== Processing Trousers Blue Denim ===\n",
      "ðŸ”„ Merging macroeconomic data for Trousers Blue Denim...\n",
      "Trousers Blue Denim | Epoch 10/50, Train Loss: 0.009456, Val Loss: 0.004517\n",
      "Trousers Blue Denim | Epoch 20/50, Train Loss: 0.008957, Val Loss: 0.003908\n",
      "Trousers Blue Denim | Epoch 30/50, Train Loss: 0.008479, Val Loss: 0.003525\n",
      "Trousers Blue Denim | Epoch 40/50, Train Loss: 0.007874, Val Loss: 0.003485\n",
      "Trousers Blue Denim | Epoch 50/50, Train Loss: 0.007209, Val Loss: 0.004041\n",
      "Trousers Blue Denim => MAE: 160.0452, RMSE: 201.0840, MAPE: 38.46%, R2: -0.0613\n",
      "\n",
      "=== Processing Vest top Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Vest top Black Solid...\n",
      "Vest top Black Solid | Epoch 10/50, Train Loss: 0.011609, Val Loss: 0.007919\n",
      "Vest top Black Solid | Epoch 20/50, Train Loss: 0.008587, Val Loss: 0.011318\n",
      "Vest top Black Solid | Epoch 30/50, Train Loss: 0.008504, Val Loss: 0.013889\n",
      "Vest top Black Solid | Epoch 40/50, Train Loss: 0.007560, Val Loss: 0.014341\n",
      "Vest top Black Solid | Epoch 50/50, Train Loss: 0.007353, Val Loss: 0.015391\n",
      "Vest top Black Solid => MAE: 355.2321, RMSE: 380.2736, MAPE: 60.24%, R2: -0.4113\n",
      "\n",
      "âœ… All TCN processing completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/macroeconomic/cpi/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Filter data for this product group\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    if product_data.empty:\n",
    "        print(f\"No data for {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure date is the index\n",
    "    product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "    product_data.set_index('date', inplace=True)\n",
    "    product_data = product_data.asfreq('D').fillna(0)\n",
    "\n",
    "    # 2. Merge Macroeconomic Data (Consumer Sentiment)\n",
    "    print(f\"ðŸ”„ Merging macroeconomic data for {product_group}...\")\n",
    "    product_data = product_data.join(cpi[['USACP030000CTGYM']], how='left')\n",
    "    product_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "\n",
    "    # 3. Drop unwanted columns\n",
    "    drop_cols = ['product_group', 'product_type_name', 'colour_group_name', 'graphical_appearance_name']\n",
    "    product_data.drop(columns=[c for c in drop_cols if c in product_data.columns], inplace=True, errors='ignore')\n",
    "\n",
    "    if 'transaction_count' not in product_data.columns:\n",
    "        print(f\"No transaction_count in {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 4. Train/Validation Split (80/20)\n",
    "    total_len = len(product_data)\n",
    "    if total_len < 50:\n",
    "        print(f\"Not enough data for {product_group}, skipping.\")\n",
    "        continue\n",
    "    split_idx = int(0.8 * total_len)\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "\n",
    "    # 5. Scale features & target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = train_df.drop(columns=['transaction_count'])\n",
    "    y_train = train_df[['transaction_count']]\n",
    "\n",
    "    X_val = val_df.drop(columns=['transaction_count'])\n",
    "    y_val = val_df[['transaction_count']]\n",
    "\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "\n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "\n",
    "    # 6. Create TimeSeriesDataset\n",
    "    input_window, output_window = 14, 1\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 7. Build TCN\n",
    "    num_features = X_train.shape[1] + 1 \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=[64, 64],\n",
    "        output_size=1,\n",
    "        kernel_size=3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    # 8. Train TCN\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"{product_group} | Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # 9. Evaluate on Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Invert scaling\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    all_groups_results[product_group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "    print(f\"{product_group} => MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual with CPI for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"metrics_summary.csv\"))\n",
    "\n",
    "# Compute and save average metrics\n",
    "avg_metrics = {\n",
    "    'MAE': np.mean([all_groups_results[pg]['MAE'] for pg in all_groups_results]),\n",
    "    'RMSE': np.mean([all_groups_results[pg]['RMSE'] for pg in all_groups_results]),\n",
    "    'MAPE': np.mean([all_groups_results[pg]['MAPE'] for pg in all_groups_results]),\n",
    "    'R2': np.mean([all_groups_results[pg]['R2'] for pg in all_groups_results])\n",
    "}\n",
    "\n",
    "avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "avg_metrics_df.to_csv(os.path.join(output_dir, \"avg_metrics.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(\"\\nâœ… All TCN processing completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_best_hyperparameters' from 'modules.utils' (/Users/megan/Thesis/modules/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_name, load_best_hyperparameters\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Output directory\u001b[39;00m\n\u001b[1;32m     14\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_version/output/macroeconomic/cpi/tcn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_best_hyperparameters' from 'modules.utils' (/Users/megan/Thesis/modules/utils.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name, load_best_hyperparameters\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/macroeconomic/cpi/tcn\"\n",
    "\n",
    "# Initialize results dictionary\n",
    "all_groups_results = {}\n",
    "\n",
    "# Train and evaluate using best hyperparameters\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Training with best hyperparameters for {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "\n",
    "    # Load best hyperparameters\n",
    "    best_params = load_best_hyperparameters(group_output_dir)\n",
    "    if best_params is None:\n",
    "        print(f\"Skipping {product_group}, no best hyperparameters found.\")\n",
    "        continue\n",
    "\n",
    "    # Load product data\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    if product_data.empty:\n",
    "        print(f\"Skipping {product_group}, no data available.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure date is the index\n",
    "    product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "    product_data.set_index('date', inplace=True)\n",
    "    product_data = product_data.asfreq('D').fillna(0)\n",
    "\n",
    "    # Train-validation split\n",
    "    total_len = len(product_data)\n",
    "    if total_len < 50:  # Ensure minimum required data points\n",
    "        print(f\"Skipping {product_group}, not enough data points.\")\n",
    "        continue\n",
    "    \n",
    "    split_idx = int(0.8 * total_len)\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "\n",
    "    # Ensure train_df is not empty\n",
    "    if train_df.empty:\n",
    "        print(f\"Skipping {product_group}, train dataset is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Scale features and target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = train_df.drop(columns=['transaction_count'], errors='ignore')\n",
    "    y_train = train_df[['transaction_count']]\n",
    "    X_val = val_df.drop(columns=['transaction_count'], errors='ignore')\n",
    "    y_val = val_df[['transaction_count']]\n",
    "\n",
    "    # Ensure X_train is not empty before scaling\n",
    "    if X_train.shape[0] == 0:\n",
    "        print(f\"Skipping {product_group}, no training samples found after filtering.\")\n",
    "        continue\n",
    "\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "\n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "\n",
    "    # Create dataset\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_df_scaled, input_window=14, output_window=1, target_col_name='transaction_count'\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_df_scaled, input_window=14, output_window=1, target_col_name='transaction_count'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Build model using best hyperparameters\n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=X_train.shape[1] + 1,\n",
    "        num_channels=[int(best_params['num_channels_1']), int(best_params['num_channels_2'])],\n",
    "        output_size=1,\n",
    "        kernel_size=int(best_params['kernel_size']),\n",
    "        dropout=best_params['dropout']\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch.permute(0, 2, 1))  # Ensure correct shape\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch.permute(0, 2, 1))  # Ensure correct shape\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Invert scaling\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    all_groups_results[product_group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "    print(f\"{product_group} => MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual with CPI for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary and average metrics\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"tcn_finetuned_summary.csv\"))\n",
    "\n",
    "# Compute and save average metrics\n",
    "avg_metrics = {\n",
    "    'MAE': np.mean([all_groups_results[pg]['MAE'] for pg in all_groups_results]),\n",
    "    'RMSE': np.mean([all_groups_results[pg]['RMSE'] for pg in all_groups_results]),\n",
    "    'MAPE': np.mean([all_groups_results[pg]['MAPE'] for pg in all_groups_results]),\n",
    "    'R2': np.mean([all_groups_results[pg]['R2'] for pg in all_groups_results])\n",
    "}\n",
    "\n",
    "avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "avg_metrics_df.to_csv(os.path.join(output_dir, \"avg_finetuned_metrics.csv\"), index=False)\n",
    "\n",
    "print(\"\\nâœ… Final TCN training with best hyperparameters completed. Metrics and predictions saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with best hyperparameters for Trousers_Black_Solid ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m X_val \u001b[38;5;241m=\u001b[39m val_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_count\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     51\u001b[0m y_val \u001b[38;5;241m=\u001b[39m val_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_count\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 53\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m y_train_scaled \u001b[38;5;241m=\u001b[39m target_scaler\u001b[38;5;241m.\u001b[39mfit_transform(y_train)\n\u001b[1;32m     55\u001b[0m X_val_scaled \u001b[38;5;241m=\u001b[39m feature_scaler\u001b[38;5;241m.\u001b[39mtransform(X_val)\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Thesis/thesis_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/macroeconomic/cpi/tcn\"\n",
    "\n",
    "def load_best_hyperparameters(group_output_dir):\n",
    "    best_params_path = os.path.join(group_output_dir, \"best_hyperparameters.csv\")\n",
    "    if not os.path.exists(best_params_path):\n",
    "        return None\n",
    "    return pd.read_csv(best_params_path).iloc[0].to_dict()\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "# Train and evaluate using best hyperparameters\n",
    "for product_group in os.listdir(output_dir):\n",
    "    group_output_dir = os.path.join(output_dir, product_group)\n",
    "    if not os.path.isdir(group_output_dir):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== Training with best hyperparameters for {product_group} ===\")\n",
    "    best_params = load_best_hyperparameters(group_output_dir)\n",
    "    if best_params is None:\n",
    "        print(f\"Skipping {product_group}, no best hyperparameters found.\")\n",
    "        continue\n",
    "\n",
    "    # Load product data\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    \n",
    "    # Train-validation split\n",
    "    split_idx = int(0.8 * len(product_data))\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "    \n",
    "    # Scale features and target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train = train_df.drop(columns=['transaction_count'])\n",
    "    y_train = train_df[['transaction_count']]\n",
    "    X_val = val_df.drop(columns=['transaction_count'])\n",
    "    y_val = val_df[['transaction_count']]\n",
    "    \n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "    \n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "    \n",
    "    # Create dataset\n",
    "    input_window, output_window = 14, 1\n",
    "    train_dataset = TimeSeriesDataset(train_df_scaled, input_window, output_window, 'transaction_count')\n",
    "    val_dataset = TimeSeriesDataset(val_df_scaled, input_window, output_window, 'transaction_count')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Load best model\n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=X_train.shape[1] + 1,\n",
    "        num_channels=[int(best_params['num_channels_1']), int(best_params['num_channels_2'])],\n",
    "        output_size=1,\n",
    "        kernel_size=int(best_params['kernel_size']),\n",
    "        dropout=best_params['dropout']\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.permute(0, 2, 1)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/50, Train Loss: {train_loss:.6f}\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.permute(0, 2, 1)\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "    \n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "    \n",
    "    all_groups_results[product_group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "    \n",
    "    print(f\"{product_group} => MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R2: {r2:.4f}\")\n",
    "    \n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual with CPI for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary and average metrics\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"tcn_trends_summary.csv\"))\n",
    "\n",
    "avg_metrics = {\n",
    "    'MAE': np.mean([all_groups_results[pg]['MAE'] for pg in all_groups_results]),\n",
    "    'RMSE': np.mean([all_groups_results[pg]['RMSE'] for pg in all_groups_results]),\n",
    "    'MAPE': np.mean([all_groups_results[pg]['MAPE'] for pg in all_groups_results]),\n",
    "    'R2': np.mean([all_groups_results[pg]['R2'] for pg in all_groups_results])\n",
    "}\n",
    "\n",
    "avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "avg_metrics_df.to_csv(os.path.join(output_dir, \"avg_metrics.csv\"), index=False)\n",
    "\n",
    "print(\"\\nâœ… Training with best hyperparameters completed. Metrics and predictions saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Bra Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Bra Black Solid...\n",
      "Bra Black Solid | Epoch 10/50, Train Loss: 0.007562, Val Loss: 0.004630\n",
      "Bra Black Solid | Epoch 20/50, Train Loss: 0.006329, Val Loss: 0.003753\n",
      "Bra Black Solid | Epoch 30/50, Train Loss: 0.006160, Val Loss: 0.003685\n",
      "Bra Black Solid | Epoch 40/50, Train Loss: 0.005870, Val Loss: 0.003947\n",
      "Bra Black Solid | Epoch 50/50, Train Loss: 0.005693, Val Loss: 0.003752\n",
      "Bra Black Solid => MAE: 146.5768, RMSE: 179.8569, MAPE: 27.92%, R2: 0.1812\n",
      "\n",
      "=== Processing Dress Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Dress Black Solid...\n",
      "Dress Black Solid | Epoch 10/50, Train Loss: 0.017544, Val Loss: 0.012691\n",
      "Dress Black Solid | Epoch 20/50, Train Loss: 0.013655, Val Loss: 0.009880\n",
      "Dress Black Solid | Epoch 30/50, Train Loss: 0.012410, Val Loss: 0.009137\n",
      "Dress Black Solid | Epoch 40/50, Train Loss: 0.011519, Val Loss: 0.008703\n",
      "Dress Black Solid | Epoch 50/50, Train Loss: 0.010095, Val Loss: 0.008257\n",
      "Dress Black Solid => MAE: 243.9804, RMSE: 372.5210, MAPE: 21.02%, R2: 0.5444\n",
      "\n",
      "=== Processing Leggings/Tights Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Leggings/Tights Black Solid...\n",
      "Leggings/Tights Black Solid | Epoch 10/50, Train Loss: 0.009815, Val Loss: 0.002891\n",
      "Leggings/Tights Black Solid | Epoch 20/50, Train Loss: 0.008409, Val Loss: 0.002393\n",
      "Leggings/Tights Black Solid | Epoch 30/50, Train Loss: 0.007355, Val Loss: 0.002043\n",
      "Leggings/Tights Black Solid | Epoch 40/50, Train Loss: 0.006476, Val Loss: 0.001973\n",
      "Leggings/Tights Black Solid | Epoch 50/50, Train Loss: 0.006853, Val Loss: 0.002057\n",
      "Leggings/Tights Black Solid => MAE: 104.8557, RMSE: 129.2551, MAPE: 31.19%, R2: 0.3660\n",
      "\n",
      "=== Processing Sweater Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Sweater Black Solid...\n",
      "Sweater Black Solid | Epoch 10/50, Train Loss: 0.005364, Val Loss: 0.002177\n",
      "Sweater Black Solid | Epoch 20/50, Train Loss: 0.003918, Val Loss: 0.001311\n",
      "Sweater Black Solid | Epoch 30/50, Train Loss: 0.003474, Val Loss: 0.001220\n",
      "Sweater Black Solid | Epoch 40/50, Train Loss: 0.003282, Val Loss: 0.001089\n",
      "Sweater Black Solid | Epoch 50/50, Train Loss: 0.003260, Val Loss: 0.000989\n",
      "Sweater Black Solid => MAE: 195.2327, RMSE: 227.2054, MAPE: 120.80%, R2: 0.5535\n",
      "\n",
      "=== Processing T-shirt Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for T-shirt Black Solid...\n",
      "T-shirt Black Solid | Epoch 10/50, Train Loss: 0.011704, Val Loss: 0.006454\n",
      "T-shirt Black Solid | Epoch 20/50, Train Loss: 0.010204, Val Loss: 0.006749\n",
      "T-shirt Black Solid | Epoch 30/50, Train Loss: 0.009196, Val Loss: 0.006137\n",
      "T-shirt Black Solid | Epoch 40/50, Train Loss: 0.008208, Val Loss: 0.005857\n",
      "T-shirt Black Solid | Epoch 50/50, Train Loss: 0.007963, Val Loss: 0.006133\n",
      "T-shirt Black Solid => MAE: 144.9214, RMSE: 182.8208, MAPE: 23.04%, R2: -0.6098\n",
      "\n",
      "=== Processing T-shirt White Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for T-shirt White Solid...\n",
      "T-shirt White Solid | Epoch 10/50, Train Loss: 0.011459, Val Loss: 0.006598\n",
      "T-shirt White Solid | Epoch 20/50, Train Loss: 0.008409, Val Loss: 0.003536\n",
      "T-shirt White Solid | Epoch 30/50, Train Loss: 0.006784, Val Loss: 0.002432\n",
      "T-shirt White Solid | Epoch 40/50, Train Loss: 0.006240, Val Loss: 0.002279\n",
      "T-shirt White Solid | Epoch 50/50, Train Loss: 0.006036, Val Loss: 0.002242\n",
      "T-shirt White Solid => MAE: 100.4999, RMSE: 133.7161, MAPE: 17.72%, R2: 0.5597\n",
      "\n",
      "=== Processing Top Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Top Black Solid...\n",
      "Top Black Solid | Epoch 10/50, Train Loss: 0.005829, Val Loss: 0.003817\n",
      "Top Black Solid | Epoch 20/50, Train Loss: 0.004974, Val Loss: 0.004008\n",
      "Top Black Solid | Epoch 30/50, Train Loss: 0.004778, Val Loss: 0.004131\n",
      "Top Black Solid | Epoch 40/50, Train Loss: 0.004641, Val Loss: 0.004234\n",
      "Top Black Solid | Epoch 50/50, Train Loss: 0.004565, Val Loss: 0.004044\n",
      "Top Black Solid => MAE: 203.9304, RMSE: 249.3524, MAPE: 28.37%, R2: -0.4316\n",
      "\n",
      "=== Processing Trousers Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Trousers Black Solid...\n",
      "Trousers Black Solid | Epoch 10/50, Train Loss: 0.008059, Val Loss: 0.001589\n",
      "Trousers Black Solid | Epoch 20/50, Train Loss: 0.007432, Val Loss: 0.001245\n",
      "Trousers Black Solid | Epoch 30/50, Train Loss: 0.007065, Val Loss: 0.001173\n",
      "Trousers Black Solid | Epoch 40/50, Train Loss: 0.006788, Val Loss: 0.001114\n",
      "Trousers Black Solid | Epoch 50/50, Train Loss: 0.006367, Val Loss: 0.001078\n",
      "Trousers Black Solid => MAE: 190.7495, RMSE: 248.7772, MAPE: 19.75%, R2: 0.2300\n",
      "\n",
      "=== Processing Trousers Blue Denim ===\n",
      "ðŸ”„ Merging macroeconomic data for Trousers Blue Denim...\n",
      "Trousers Blue Denim | Epoch 10/50, Train Loss: 0.009483, Val Loss: 0.004395\n",
      "Trousers Blue Denim | Epoch 20/50, Train Loss: 0.008195, Val Loss: 0.004164\n",
      "Trousers Blue Denim | Epoch 30/50, Train Loss: 0.007872, Val Loss: 0.004154\n",
      "Trousers Blue Denim | Epoch 40/50, Train Loss: 0.007568, Val Loss: 0.004115\n",
      "Trousers Blue Denim | Epoch 50/50, Train Loss: 0.007565, Val Loss: 0.003869\n",
      "Trousers Blue Denim => MAE: 158.2950, RMSE: 190.9835, MAPE: 35.57%, R2: 0.0426\n",
      "\n",
      "=== Processing Vest top Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Vest top Black Solid...\n",
      "Vest top Black Solid | Epoch 10/50, Train Loss: 0.015646, Val Loss: 0.005172\n",
      "Vest top Black Solid | Epoch 20/50, Train Loss: 0.011051, Val Loss: 0.004164\n",
      "Vest top Black Solid | Epoch 30/50, Train Loss: 0.010104, Val Loss: 0.005170\n",
      "Vest top Black Solid | Epoch 40/50, Train Loss: 0.008420, Val Loss: 0.006060\n",
      "Vest top Black Solid | Epoch 50/50, Train Loss: 0.007934, Val Loss: 0.006962\n",
      "Vest top Black Solid => MAE: 231.3703, RMSE: 276.9312, MAPE: 35.84%, R2: 0.2515\n",
      "\n",
      "âœ… All TCN processing completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/macroeconomic/gdp/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Filter data for this product group\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    if product_data.empty:\n",
    "        print(f\"No data for {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure date is the index\n",
    "    product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "    product_data.set_index('date', inplace=True)\n",
    "    product_data = product_data.asfreq('D').fillna(0)\n",
    "\n",
    "    # 2. Merge Macroeconomic Data (Consumer Sentiment)\n",
    "    print(f\"ðŸ”„ Merging macroeconomic data for {product_group}...\")\n",
    "    product_data = product_data.join(gdp[['GDP']], how='left')\n",
    "    product_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "\n",
    "    # 3. Drop unwanted columns\n",
    "    drop_cols = ['product_group', 'product_type_name', 'colour_group_name', 'graphical_appearance_name']\n",
    "    product_data.drop(columns=[c for c in drop_cols if c in product_data.columns], inplace=True, errors='ignore')\n",
    "\n",
    "    if 'transaction_count' not in product_data.columns:\n",
    "        print(f\"No transaction_count in {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 4. Train/Validation Split (80/20)\n",
    "    total_len = len(product_data)\n",
    "    if total_len < 50:\n",
    "        print(f\"Not enough data for {product_group}, skipping.\")\n",
    "        continue\n",
    "    split_idx = int(0.8 * total_len)\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "\n",
    "    # 5. Scale features & target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = train_df.drop(columns=['transaction_count'])\n",
    "    y_train = train_df[['transaction_count']]\n",
    "\n",
    "    X_val = val_df.drop(columns=['transaction_count'])\n",
    "    y_val = val_df[['transaction_count']]\n",
    "\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "\n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "\n",
    "    # 6. Create TimeSeriesDataset\n",
    "    input_window, output_window = 14, 1\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 7. Build TCN\n",
    "    num_features = X_train.shape[1] + 1 \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=[64, 64],\n",
    "        output_size=1,\n",
    "        kernel_size=3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    # 8. Train TCN\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"{product_group} | Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # 9. Evaluate on Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Invert scaling\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    all_groups_results[product_group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "    print(f\"{product_group} => MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual with GDP for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"metrics_summary.csv\"))\n",
    "\n",
    "# Compute and save average metrics\n",
    "avg_metrics = {\n",
    "    'MAE': np.mean([all_groups_results[pg]['MAE'] for pg in all_groups_results]),\n",
    "    'RMSE': np.mean([all_groups_results[pg]['RMSE'] for pg in all_groups_results]),\n",
    "    'MAPE': np.mean([all_groups_results[pg]['MAPE'] for pg in all_groups_results]),\n",
    "    'R2': np.mean([all_groups_results[pg]['R2'] for pg in all_groups_results])\n",
    "}\n",
    "\n",
    "avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "avg_metrics_df.to_csv(os.path.join(output_dir, \"avg_metrics.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(\"\\nâœ… All TCN processing completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import optuna\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/macroeconomic/gdp/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def objective_fn(trial, train_loader, val_loader, num_features):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 2, 5)\n",
    "    num_channels = [trial.suggest_int(\"num_channels_1\", 32, 128), trial.suggest_int(\"num_channels_2\", 32, 128)]\n",
    "    \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=num_channels,\n",
    "        output_size=1,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(20):  # Reduced epochs for tuning\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "# Run Optuna for all product groups\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run Optuna hyperparameter tuning\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective_fn(trial, train_loader, val_loader, X_train.shape[1] + 1), n_trials=30)\n",
    "    \n",
    "    # Save best hyperparameters\n",
    "    best_params = study.best_trial.params\n",
    "    pd.DataFrame([best_params]).to_csv(os.path.join(group_output_dir, \"best_hyperparameters.csv\"), index=False)\n",
    "    \n",
    "    print(f\"âœ… Best hyperparameters for {product_group}: {best_params}\")\n",
    "    \n",
    "    # Train model with best hyperparameters\n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=X_train.shape[1] + 1,\n",
    "        num_channels=[int(best_params['num_channels_1']), int(best_params['num_channels_2'])],\n",
    "        output_size=1,\n",
    "        kernel_size=int(best_params['kernel_size']),\n",
    "        dropout=best_params['dropout']\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Save trained model\n",
    "    torch.save(model.state_dict(), os.path.join(group_output_dir, \"best_model.pth\"))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Compute metrics\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(group_output_dir, \"test_metrics.csv\"), index=False)\n",
    "    all_groups_results[product_group] = metrics\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary and average metrics\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"tcn_trends_summary.csv\"))\n",
    "\n",
    "avg_metrics = {\n",
    "    'MAE': np.mean([all_groups_results[pg]['MAE'] for pg in all_groups_results]),\n",
    "    'RMSE': np.mean([all_groups_results[pg]['RMSE'] for pg in all_groups_results]),\n",
    "    'MAPE': np.mean([all_groups_results[pg]['MAPE'] for pg in all_groups_results]),\n",
    "    'R2': np.mean([all_groups_results[pg]['R2'] for pg in all_groups_results])\n",
    "}\n",
    "\n",
    "pd.DataFrame([avg_metrics]).to_csv(os.path.join(output_dir, \"avg_metrics.csv\"), index=False)\n",
    "\n",
    "print(\"\\nâœ… All CPI TCN processing with Optuna tuning, best model training, evaluation, and summary completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Bra Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Bra Black Solid...\n",
      "Bra Black Solid | Epoch 10/50, Train Loss: 0.008869, Val Loss: 0.004915\n",
      "Bra Black Solid | Epoch 20/50, Train Loss: 0.007986, Val Loss: 0.005637\n",
      "Bra Black Solid | Epoch 30/50, Train Loss: 0.007189, Val Loss: 0.005532\n",
      "Bra Black Solid | Epoch 40/50, Train Loss: 0.006897, Val Loss: 0.005186\n",
      "Bra Black Solid | Epoch 50/50, Train Loss: 0.006564, Val Loss: 0.005222\n",
      "Bra Black Solid => MAE: 176.1876, RMSE: 206.0615, MAPE: 34.47%, R2: -0.0747\n",
      "\n",
      "=== Processing Dress Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Dress Black Solid...\n",
      "Dress Black Solid | Epoch 10/50, Train Loss: 0.015863, Val Loss: 0.016913\n",
      "Dress Black Solid | Epoch 20/50, Train Loss: 0.014175, Val Loss: 0.013848\n",
      "Dress Black Solid | Epoch 30/50, Train Loss: 0.012146, Val Loss: 0.010948\n",
      "Dress Black Solid | Epoch 40/50, Train Loss: 0.012206, Val Loss: 0.010110\n",
      "Dress Black Solid | Epoch 50/50, Train Loss: 0.011590, Val Loss: 0.009620\n",
      "Dress Black Solid => MAE: 280.0306, RMSE: 401.9834, MAPE: 25.59%, R2: 0.4695\n",
      "\n",
      "=== Processing Leggings/Tights Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Leggings/Tights Black Solid...\n",
      "Leggings/Tights Black Solid | Epoch 10/50, Train Loss: 0.007373, Val Loss: 0.004355\n",
      "Leggings/Tights Black Solid | Epoch 20/50, Train Loss: 0.006129, Val Loss: 0.003337\n",
      "Leggings/Tights Black Solid | Epoch 30/50, Train Loss: 0.006158, Val Loss: 0.003067\n",
      "Leggings/Tights Black Solid | Epoch 40/50, Train Loss: 0.005554, Val Loss: 0.002688\n",
      "Leggings/Tights Black Solid | Epoch 50/50, Train Loss: 0.005205, Val Loss: 0.002718\n",
      "Leggings/Tights Black Solid => MAE: 123.1929, RMSE: 145.5439, MAPE: 35.32%, R2: 0.1961\n",
      "\n",
      "=== Processing Sweater Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Sweater Black Solid...\n",
      "Sweater Black Solid | Epoch 10/50, Train Loss: 0.006587, Val Loss: 0.002172\n",
      "Sweater Black Solid | Epoch 20/50, Train Loss: 0.005510, Val Loss: 0.001357\n",
      "Sweater Black Solid | Epoch 30/50, Train Loss: 0.004257, Val Loss: 0.001064\n",
      "Sweater Black Solid | Epoch 40/50, Train Loss: 0.003761, Val Loss: 0.001162\n",
      "Sweater Black Solid | Epoch 50/50, Train Loss: 0.004037, Val Loss: 0.001132\n",
      "Sweater Black Solid => MAE: 155.5639, RMSE: 219.0356, MAPE: 67.78%, R2: 0.5851\n",
      "\n",
      "=== Processing T-shirt Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for T-shirt Black Solid...\n",
      "T-shirt Black Solid | Epoch 10/50, Train Loss: 0.011998, Val Loss: 0.005082\n",
      "T-shirt Black Solid | Epoch 20/50, Train Loss: 0.010156, Val Loss: 0.004916\n",
      "T-shirt Black Solid | Epoch 30/50, Train Loss: 0.010081, Val Loss: 0.004828\n",
      "T-shirt Black Solid | Epoch 40/50, Train Loss: 0.009339, Val Loss: 0.004412\n",
      "T-shirt Black Solid | Epoch 50/50, Train Loss: 0.008176, Val Loss: 0.004045\n",
      "T-shirt Black Solid => MAE: 117.6936, RMSE: 153.8645, MAPE: 18.79%, R2: -0.1403\n",
      "\n",
      "=== Processing T-shirt White Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for T-shirt White Solid...\n",
      "T-shirt White Solid | Epoch 10/50, Train Loss: 0.010019, Val Loss: 0.002936\n",
      "T-shirt White Solid | Epoch 20/50, Train Loss: 0.007949, Val Loss: 0.002909\n",
      "T-shirt White Solid | Epoch 30/50, Train Loss: 0.007354, Val Loss: 0.003308\n",
      "T-shirt White Solid | Epoch 40/50, Train Loss: 0.006530, Val Loss: 0.002957\n",
      "T-shirt White Solid | Epoch 50/50, Train Loss: 0.005829, Val Loss: 0.003097\n",
      "T-shirt White Solid => MAE: 125.3394, RMSE: 148.6824, MAPE: 25.72%, R2: 0.4556\n",
      "\n",
      "=== Processing Top Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Top Black Solid...\n",
      "Top Black Solid | Epoch 10/50, Train Loss: 0.006356, Val Loss: 0.004306\n",
      "Top Black Solid | Epoch 20/50, Train Loss: 0.005543, Val Loss: 0.003292\n",
      "Top Black Solid | Epoch 30/50, Train Loss: 0.004773, Val Loss: 0.003041\n",
      "Top Black Solid | Epoch 40/50, Train Loss: 0.004615, Val Loss: 0.003195\n",
      "Top Black Solid | Epoch 50/50, Train Loss: 0.004529, Val Loss: 0.003201\n",
      "Top Black Solid => MAE: 186.2947, RMSE: 228.6093, MAPE: 25.57%, R2: -0.2033\n",
      "\n",
      "=== Processing Trousers Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Trousers Black Solid...\n",
      "Trousers Black Solid | Epoch 10/50, Train Loss: 0.007653, Val Loss: 0.001856\n",
      "Trousers Black Solid | Epoch 20/50, Train Loss: 0.007094, Val Loss: 0.001759\n",
      "Trousers Black Solid | Epoch 30/50, Train Loss: 0.006513, Val Loss: 0.001463\n",
      "Trousers Black Solid | Epoch 40/50, Train Loss: 0.006173, Val Loss: 0.001301\n",
      "Trousers Black Solid | Epoch 50/50, Train Loss: 0.005920, Val Loss: 0.001362\n",
      "Trousers Black Solid => MAE: 196.8990, RMSE: 263.4496, MAPE: 18.77%, R2: 0.1365\n",
      "\n",
      "=== Processing Trousers Blue Denim ===\n",
      "ðŸ”„ Merging macroeconomic data for Trousers Blue Denim...\n",
      "Trousers Blue Denim | Epoch 10/50, Train Loss: 0.009914, Val Loss: 0.004296\n",
      "Trousers Blue Denim | Epoch 20/50, Train Loss: 0.008949, Val Loss: 0.004143\n",
      "Trousers Blue Denim | Epoch 30/50, Train Loss: 0.008648, Val Loss: 0.004181\n",
      "Trousers Blue Denim | Epoch 40/50, Train Loss: 0.008153, Val Loss: 0.004079\n",
      "Trousers Blue Denim | Epoch 50/50, Train Loss: 0.007178, Val Loss: 0.004270\n",
      "Trousers Blue Denim => MAE: 169.3578, RMSE: 205.7034, MAPE: 39.72%, R2: -0.1106\n",
      "\n",
      "=== Processing Vest top Black Solid ===\n",
      "ðŸ”„ Merging macroeconomic data for Vest top Black Solid...\n",
      "Vest top Black Solid | Epoch 10/50, Train Loss: 0.012447, Val Loss: 0.007280\n",
      "Vest top Black Solid | Epoch 20/50, Train Loss: 0.009456, Val Loss: 0.006109\n",
      "Vest top Black Solid | Epoch 30/50, Train Loss: 0.008122, Val Loss: 0.005769\n",
      "Vest top Black Solid | Epoch 40/50, Train Loss: 0.007903, Val Loss: 0.005704\n",
      "Vest top Black Solid | Epoch 50/50, Train Loss: 0.007255, Val Loss: 0.005611\n",
      "Vest top Black Solid => MAE: 211.6228, RMSE: 247.2993, MAPE: 33.06%, R2: 0.4031\n",
      "\n",
      "âœ… All TCN processing completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from modules.utils import process_name\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"final_version/output/macroeconomic/unemployment/tcn\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "all_groups_results = {}\n",
    "\n",
    "for product_group in top_10_groups:\n",
    "    print(f\"\\n=== Processing {product_group} ===\")\n",
    "    sanitized_group = process_name(product_group)\n",
    "    group_output_dir = os.path.join(output_dir, sanitized_group)\n",
    "    os.makedirs(group_output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Filter data for this product group\n",
    "    product_data = data[data['product_group'] == product_group].copy()\n",
    "    if product_data.empty:\n",
    "        print(f\"No data for {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure date is the index\n",
    "    product_data['date'] = pd.to_datetime(product_data['date'], errors='coerce')\n",
    "    product_data.set_index('date', inplace=True)\n",
    "    product_data = product_data.asfreq('D').fillna(0)\n",
    "\n",
    "    # 2. Merge Macroeconomic Data (Consumer Sentiment)\n",
    "    print(f\"ðŸ”„ Merging macroeconomic data for {product_group}...\")\n",
    "    product_data = product_data.join(unemployment[['UNRATE']], how='left')\n",
    "    product_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "\n",
    "    # 3. Drop unwanted columns\n",
    "    drop_cols = ['product_group', 'product_type_name', 'colour_group_name', 'graphical_appearance_name']\n",
    "    product_data.drop(columns=[c for c in drop_cols if c in product_data.columns], inplace=True, errors='ignore')\n",
    "\n",
    "    if 'transaction_count' not in product_data.columns:\n",
    "        print(f\"No transaction_count in {product_group}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 4. Train/Validation Split (80/20)\n",
    "    total_len = len(product_data)\n",
    "    if total_len < 50:\n",
    "        print(f\"Not enough data for {product_group}, skipping.\")\n",
    "        continue\n",
    "    split_idx = int(0.8 * total_len)\n",
    "    train_df = product_data.iloc[:split_idx].copy()\n",
    "    val_df = product_data.iloc[split_idx:].copy()\n",
    "\n",
    "    # 5. Scale features & target\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = train_df.drop(columns=['transaction_count'])\n",
    "    y_train = train_df[['transaction_count']]\n",
    "\n",
    "    X_val = val_df.drop(columns=['transaction_count'])\n",
    "    y_val = val_df[['transaction_count']]\n",
    "\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train)\n",
    "\n",
    "    X_val_scaled = feature_scaler.transform(X_val)\n",
    "    y_val_scaled = target_scaler.transform(y_val)\n",
    "\n",
    "    train_df_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    train_df_scaled['transaction_count'] = y_train_scaled\n",
    "\n",
    "    val_df_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "    val_df_scaled['transaction_count'] = y_val_scaled\n",
    "\n",
    "    # 6. Create TimeSeriesDataset\n",
    "    input_window, output_window = 14, 1\n",
    "    train_dataset = TimeSeriesDataset(\n",
    "        train_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataset(\n",
    "        val_df_scaled,\n",
    "        input_window=input_window,\n",
    "        output_window=output_window,\n",
    "        target_col_name='transaction_count'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 7. Build TCN\n",
    "    num_features = X_train.shape[1] + 1 \n",
    "    model = TCNForecastingModel(\n",
    "        num_inputs=num_features,\n",
    "        num_channels=[64, 64],\n",
    "        output_size=1,\n",
    "        kernel_size=3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    # 8. Train TCN\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"{product_group} | Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # 9. Evaluate on Validation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            preds = model(X_batch)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Invert scaling\n",
    "    preds_unscaled = target_scaler.inverse_transform(all_preds)\n",
    "    targets_unscaled = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mae = mean_absolute_error(targets_unscaled, preds_unscaled)\n",
    "    rmse = np.sqrt(mean_squared_error(targets_unscaled, preds_unscaled))\n",
    "    mape = np.mean(np.abs((targets_unscaled - preds_unscaled) / np.maximum(targets_unscaled, 1))) * 100\n",
    "    r2 = r2_score(targets_unscaled, preds_unscaled)\n",
    "\n",
    "    all_groups_results[product_group] = {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
    "\n",
    "    print(f\"{product_group} => MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save Predictions vs Actual Graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(targets_unscaled, label=\"Actual\")\n",
    "    plt.plot(preds_unscaled, label=\"Predicted\", linestyle='--')\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Transaction Count\")\n",
    "    plt.title(f\"TCN Predictions vs Actual with Unemployment Rate for {product_group}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(group_output_dir, \"predictions_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame.from_dict(all_groups_results, orient='index')\n",
    "summary_df.to_csv(os.path.join(output_dir, \"metrics_summary.csv\"))\n",
    "\n",
    "# Compute and save average metrics\n",
    "avg_metrics = {\n",
    "    'MAE': np.mean([all_groups_results[pg]['MAE'] for pg in all_groups_results]),\n",
    "    'RMSE': np.mean([all_groups_results[pg]['RMSE'] for pg in all_groups_results]),\n",
    "    'MAPE': np.mean([all_groups_results[pg]['MAPE'] for pg in all_groups_results]),\n",
    "    'R2': np.mean([all_groups_results[pg]['R2'] for pg in all_groups_results])\n",
    "}\n",
    "\n",
    "avg_metrics_df = pd.DataFrame([avg_metrics])\n",
    "avg_metrics_df.to_csv(os.path.join(output_dir, \"avg_metrics.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(\"\\nâœ… All TCN processing completed. Results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
